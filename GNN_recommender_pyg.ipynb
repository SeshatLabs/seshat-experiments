{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSTALATION #######\n",
    "pip uninstall torch -y\n",
    "pip install torch==1.13.1\n",
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### IMPORT #######\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17310, 8023])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "                               name   \n",
      "itemId                                \n",
      "1                       TetherToken  \\\n",
      "2       TransparentUpgradeableProxy   \n",
      "3                       BridgeToken   \n",
      "4                             Token   \n",
      "5                           Seaport   \n",
      "...                             ...   \n",
      "17306                    BabyBirdez   \n",
      "17307                        OGCATS   \n",
      "17308                    ConcaveNFT   \n",
      "17309                       XPlanet   \n",
      "17310                     AstroGrow   \n",
      "\n",
      "                                                 keywords   \n",
      "itemId                                                      \n",
      "1       dev, erc20, upgraded, address, oncode, title, ...  \\\n",
      "2       proxy, admin, proxyadmin, implementation, dedi...   \n",
      "3       beacon, _implementation_slot, erc1967, newimpl...   \n",
      "4       dev, token, ierc20, supply, erc20, allowance, ...   \n",
      "5       conduit, consideration, orders, order, 0age, i...   \n",
      "...                                                   ...   \n",
      "17306   breeding, allowed, address, contract, 00, 000,...   \n",
      "17307   unix, earnings, airdrops, giveaways, withdrawa...   \n",
      "17308   checks, mint, _mintamount, active, occur, colo...   \n",
      "17309   phase, allow, whitelisted, crew, minting, peop...   \n",
      "17310   safemath, library, authorization, owner, addre...   \n",
      "\n",
      "                                       truncated_keywords  \n",
      "itemId                                                     \n",
      "1       dev, erc20, upgraded, address, oncode, title, ...  \n",
      "2       proxy, admin, proxyadmin, implementation, dedi...  \n",
      "3       beacon, _implementation_slot, erc1967, newimpl...  \n",
      "4       dev, token, ierc20, supply, erc20, allowance, ...  \n",
      "5       conduit, consideration, orders, order, 0age, i...  \n",
      "...                                                   ...  \n",
      "17306   breeding, allowed, address, contract, 00, 000,...  \n",
      "17307   unix, earnings, airdrops, giveaways, withdrawa...  \n",
      "17308   checks, mint, _mintamount, active, occur, colo...  \n",
      "17309   phase, allow, whitelisted, crew, minting, peop...  \n",
      "17310   safemath, library, authorization, owner, addre...  \n",
      "\n",
      "[17310 rows x 3 columns]\n",
      "         00   000   0000   000000000   000000000000000000   \n",
      "itemId                                                      \n",
      "1         0     0      0           0                    0  \\\n",
      "2         0     0      0           0                    0   \n",
      "3         0     0      0           0                    0   \n",
      "4         0     0      0           0                    0   \n",
      "5         0     0      0           0                    0   \n",
      "...     ...   ...    ...         ...                  ...   \n",
      "17306     1     1      1           1                    1   \n",
      "17307     0     0      0           0                    0   \n",
      "17308     0     0      0           0                    0   \n",
      "17309     1     0      0           0                    0   \n",
      "17310     0     0      0           0                    0   \n",
      "\n",
      "         000000000000000000000000000000000000000000000000000000000004cce0   \n",
      "itemId                                                                      \n",
      "1                                                       0                  \\\n",
      "2                                                       0                   \n",
      "3                                                       0                   \n",
      "4                                                       0                   \n",
      "5                                                       0                   \n",
      "...                                                   ...                   \n",
      "17306                                                   1                   \n",
      "17307                                                   0                   \n",
      "17308                                                   0                   \n",
      "17309                                                   0                   \n",
      "17310                                                   0                   \n",
      "\n",
      "         000000004294967296   0001   000281474976710656   00100000  ...   \n",
      "itemId                                                              ...   \n",
      "1                         0      0                    0          0  ...  \\\n",
      "2                         0      0                    0          0  ...   \n",
      "3                         0      0                    0          0  ...   \n",
      "4                         0      0                    0          0  ...   \n",
      "5                         0      0                    0          0  ...   \n",
      "...                     ...    ...                  ...        ...  ...   \n",
      "17306                     0      0                    0          0  ...   \n",
      "17307                     0      0                    0          0  ...   \n",
      "17308                     0      0                    0          0  ...   \n",
      "17309                     0      0                    0          0  ...   \n",
      "17310                     0      0                    0          0  ...   \n",
      "\n",
      "        zombie  zone  zoo  zora  zx  контракт  контракта  транзакцию  遊戯苑  ꁒꍟ  \n",
      "itemId                                                                         \n",
      "1            0     0    0     0   0         0          0           0    0   0  \n",
      "2            0     0    0     0   0         0          0           0    0   0  \n",
      "3            0     0    0     0   0         0          0           0    0   0  \n",
      "4            0     0    0     0   0         0          0           0    0   0  \n",
      "5            0     0    0     0   0         0          0           0    0   0  \n",
      "...        ...   ...  ...   ...  ..       ...        ...         ...  ...  ..  \n",
      "17306        0     0    0     0   0         0          0           0    0   0  \n",
      "17307        0     0    0     0   0         0          0           0    0   0  \n",
      "17308        0     0    0     0   0         0          0           0    0   0  \n",
      "17309        0     0    0     0   0         0          0           0    0   0  \n",
      "17310        0     0    0     0   0         0          0           0    0   0  \n",
      "\n",
      "[17310 rows x 8023 columns]\n",
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "                                       userId  mappedID\n",
      "0  0xc23d11e5ae70a680f6f3e278503007dbae4b868c         0\n",
      "1  0x365782e192de5abdd3183da25d2e94a2641a616d         1\n",
      "2  0xe03795684f8be6c958ea8c389be07ecd4eb89848         2\n",
      "3  0x3811b30aff6af58314bb8864d0f455a4089c6331         3\n",
      "4  0xb6a990a19b69ba982f0b4a1bebbce212e337da9f         4\n",
      "\n",
      "Mapping of item IDs to consecutive values:\n",
      "===========================================\n",
      "                        itemId  mappedID\n",
      "0                  TetherToken         0\n",
      "1  TransparentUpgradeableProxy         1\n",
      "2                  BridgeToken         2\n",
      "3                        Token         3\n",
      "4                      Seaport         4\n",
      "\n",
      "Final edge indices pointing from users to items:\n",
      "=================================================\n",
      "tensor([[    0,     1,     1,  ..., 10760, 10760, 10760],\n",
      "        [    0,     1,     2,  ...,   669, 15766,    59]])\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of item IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to items:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "def movie_loader():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "    extract_zip(download_url(url, '.'), '.')\n",
    "    movies_path = './ml-latest-small/movies.csv'\n",
    "    ratings_path = './ml-latest-small/ratings.csv'\n",
    "    items_ratings_df = pd.read_csv(ratings_path)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    items_df = pd.read_csv(movies_path)\n",
    "    items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "    items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "    items_df = items_df.sort_values('mappedID') # (Just the last 20 movies have NaN mappedId)\n",
    "    genres = items_df['genres'].str.get_dummies('|')\n",
    "    print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "    item_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "    assert item_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat\n",
    "\n",
    "def contract_loader():\n",
    "    items_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "    items_ratings_df = items_ratings_df[:100000]\n",
    "    items_df = {}\n",
    "    items_df['name'] = items_ratings_df['item'].unique()\n",
    "    items_df['itemId'], unique_names = pd.factorize(items_df['name'])\n",
    "    items_df['itemId'] = items_df['itemId'] + 1\n",
    "    items_df = pd.DataFrame(items_df, columns=['itemId', 'name'])\n",
    "\n",
    "    contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "    contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "    contracts_df_top_words = items_df.merge(contract_top_words_df, on='name', how='left')\n",
    "    contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "    items_df = contracts_df_top_words\n",
    "    items_df.set_index('itemId', inplace=True)\n",
    "    # f =5\n",
    "    items_df['truncated_keywords'] = items_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "    X_df = items_df['truncated_keywords'].str.get_dummies(',')\n",
    "    item_feat = torch.from_numpy(X_df.values).to(torch.float)\n",
    "    print(item_feat.shape)\n",
    "    print(item_feat)\n",
    "    print(items_df)\n",
    "    print(X_df)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat\n",
    "\n",
    "universal_mode = 'contract'\n",
    "loaders = {\n",
    "    'contract_loader': contract_loader,\n",
    "    'movie_loader': movie_loader,\n",
    "}\n",
    "unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat = loaders[f'{universal_mode}_loader']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LINK BINARY PRED MODEL ##########\n",
    "def train_test_generator(unique_user_id, items_df, item_feat, edge_index_user_to_item):  \n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0,\n",
    "        num_test=0.2,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return data, train_data, test_data\n",
    "\n",
    "def GNN_recommender(data, train_data):\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(item_feat.shape[1], hidden_channels)\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## AUC EVAL VALIDATION #########\n",
    "    # edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    # edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # # val_data has neg samples in it\n",
    "    # val_loader = LinkNeighborLoader(\n",
    "    #     data=val_data,\n",
    "    #     num_neighbors=[20, 10],\n",
    "    #     edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "    #     edge_label=edge_label,\n",
    "    #     batch_size=3 * 128,\n",
    "    #     shuffle=False,\n",
    "    # )\n",
    "    # sampled_data = next(iter(val_loader))\n",
    "    # preds = []\n",
    "    # ground_truths = []\n",
    "    # for sampled_data in tqdm(val_loader):\n",
    "    #     with torch.no_grad():\n",
    "    #         sampled_data.to(device)\n",
    "    #         preds.append(model(sampled_data))\n",
    "    #         ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    # pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    # ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    # auc = roc_auc_score(ground_truth, pred)\n",
    "    # print()\n",
    "    # print(f\"Validation AUC: {auc:.4f}\")\n",
    "    # return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TRAIN TEST GENERAION ############ (Ablation here)\n",
    "\n",
    "### ABLATION EXPRIMENT ### uncomment below for hiding item_features\n",
    "# item_feat = torch.zeros_like(item_feat)\n",
    "\n",
    "data, train_data, test_data = train_test_generator(unique_user_id, items_df, item_feat, edge_index_user_to_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CSP-user\n",
      "1 7469\n",
      "2 10761\n",
      "3 3292\n",
      "tensor([ True,  True, False,  ..., False, False,  True])\n",
      "test to train ratio CSP-user 0.0983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### EXPRIMENTS #######\n",
    "### CSP #### note: if the ratio==1, rerun from the first step\n",
    "def csp_test_gen(train_data, test_data, unique_data, entity_index, experiment_abbr):\n",
    "    print(entity_index)\n",
    "    print(experiment_abbr)\n",
    "    train_data_unique_entities = set(train_data['user', 'rates', 'item'].edge_label_index[entity_index].unique().numpy())\n",
    "    print('1', len(train_data_unique_entities))\n",
    "    unique_entities = set(unique_data['mappedID'].unique())\n",
    "    print('2', len(unique_entities))\n",
    "    entities_not_in_train = unique_entities - train_data_unique_entities\n",
    "    print('3', len(entities_not_in_train))\n",
    "    mask = torch.tensor([entity in entities_not_in_train for entity in test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index].numpy()])\n",
    "    print(mask)\n",
    "    \n",
    "    test_data_filtered = copy.deepcopy(test_data)\n",
    "    test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[:, mask]\n",
    "    test_data_filtered[\"user\", \"rates\", \"item\"].edge_label = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label[mask]\n",
    "    \n",
    "    ratio = len(test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[entity_index]) / len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index])\n",
    "    print(f'test to train ratio {experiment_abbr}', ratio)\n",
    "    \n",
    "    return test_data_filtered, ratio\n",
    "\n",
    "experiment = 'ucsp'\n",
    "test_data_csp, test_to_train_ratio_csp = csp_test_gen(\n",
    "    train_data, test_data, unique_user_id, 0 if experiment == 'ucsp' else 1, 'CSP-user' if experiment == 'ucsp' else 'CSP-item'\n",
    ")\n",
    "\n",
    "# TODO Diversity\n",
    "\n",
    "# TODO Contract Representation (effect of f top-keywords in contract_feat)\n",
    "\n",
    "# TODO Sparsity (keep users just with > h interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test edges shape BEFORE adding all possible user item pairs torch.Size([2, 2017])\n",
      "test edges shape AFTER adding all possible user item pairs torch.Size([2, 930677])\n",
      "unique test users 1387\n",
      "unique test items 671\n"
     ]
    }
   ],
   "source": [
    "######## ALL_TO_ALL USER_ITEM PAIRS GENERATOR IN TEST_DATA #########\n",
    "\n",
    "# If mode GNN run below\n",
    "### SLICING TEST_DATA FOR ALL_TO_ALL EVAL ###\n",
    "slice_rate = 0.5\n",
    "if experiment == 'ucsp' or experiment == 'icsp': \n",
    "    slice_rate = 1\n",
    "    test_data = test_data_csp\n",
    "\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label_index = test_data[\"user\", \"rates\", \"item\"].edge_label_index[:, : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0]))]\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label = test_data[\"user\", \"rates\", \"item\"].edge_label[ : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label))]\n",
    "\n",
    "edge_index_test = set(zip(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].numpy(), test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].numpy()))\n",
    "\n",
    "all_users = test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].unique().numpy()\n",
    "all_items = test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy()\n",
    "\n",
    "new_edges = []\n",
    "new_labels = []\n",
    "\n",
    "#TODO instead of all possible pairs, we can continue for each user if it reaches to x samples (pos + neg)\n",
    "for user_id in all_users:\n",
    "    for item_id in all_items:\n",
    "        if (user_id, item_id) not in edge_index_test:\n",
    "            new_edges.append((user_id, item_id))\n",
    "            new_labels.append(0)\n",
    "\n",
    "import copy\n",
    "test_data_all2all = copy.deepcopy(test_data)\n",
    "\n",
    "if new_edges:\n",
    "    new_edges_tensor = torch.tensor(new_edges, dtype=torch.int64).t().contiguous()\n",
    "    new_labels_tensor = torch.tensor(new_labels, dtype=torch.int64)\n",
    "\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index, new_edges_tensor), dim=1)\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label, new_labels_tensor), dim=0)\n",
    "\n",
    "print('test edges shape BEFORE adding all possible user item pairs', test_data[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "print('test edges shape AFTER adding all possible user item pairs', test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "\n",
    "print('unique test users', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[0].unique()))\n",
    "print('unique test items', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[1].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:30<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.2840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.2307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.1940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## GNN TRAINING ############\n",
    "#if model_mode == GNN run below\n",
    "model = GNN_recommender(data, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2424/2424 [06:16<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ground truth len 930677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######## GNN PRED FOR TEST_DATA_PRIME ######### for CSP chnage the TEST_DATA_PRIME\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data_all2all,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"rates\", \"item\"), test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index),\n",
    "    edge_label=test_data_all2all[\"user\", \"rates\", \"item\"].edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "sampled_data = next(iter(test_loader))\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "pred_gnn = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth_gnn = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "print('all ground truth len', len(ground_truth_gnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR MF MODELS #############\n",
    "\n",
    "# if model_mode == 'MF':\n",
    "test_df_index = test_data_all2all['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "test_df_label = test_data_all2all['user', 'rates', 'item'].edge_label.numpy()\n",
    "test_df_index = test_df_index.T \n",
    "test_df = pd.DataFrame(test_df_index, columns=['user', 'item'])\n",
    "test_df['rating'] = test_df_label\n",
    "\n",
    "train_df_index = train_data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_df_label = train_data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_df_index = train_df_index.T \n",
    "train_df = pd.DataFrame(train_df_index, columns=['user', 'item'])\n",
    "train_df['rating'] = train_df_label\n",
    "\n",
    "def add_topic(df, contract_to_topic_df, unique_item_id):\n",
    "    item_to_topic = pd.Series(contract_to_topic_df['most_probable_topic'].values, index=contract_to_topic_df['contract_name']).to_dict()\n",
    "    mappedID_to_itemId = pd.Series(unique_item_id['itemId'].values, index=unique_item_id['mappedID']).to_dict()\n",
    "    df['item_name'] = df['item'].map(mappedID_to_itemId)\n",
    "    df['topic'] = df['item_name'].map(item_to_topic).fillna(0).astype(int)\n",
    "    df = df.drop(columns=['item_name'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "contract_to_topic_df = pd.read_parquet(\"dataset/contract_name_topic.parquet\")\n",
    "test_df = add_topic(test_df, contract_to_topic_df, unique_item_id)\n",
    "train_df = add_topic(train_df, contract_to_topic_df, unique_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:02<00:00, 659.67it/s]\n"
     ]
    }
   ],
   "source": [
    "######## NAME LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "# dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids = np.union1d(train_df['user'].unique(), test_df['user'].unique())\n",
    "item_ids = np.union1d(train_df['item'].unique(), test_df['item'].unique())\n",
    "dataset.fit(user_ids, item_ids)\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "test_df['pred_nmf'] = 0\n",
    "\n",
    "for user, user_data in tqdm(test_df.groupby('user'), total=test_df['user'].nunique()):\n",
    "    user_id_internal = user_ids_mapping[user]\n",
    "    item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['item']])\n",
    "    predictions = model.predict(user_id_internal, item_ids_internal)\n",
    "    test_df.loc[user_data.index, 'pred_nmf'] = predictions\n",
    "\n",
    "pred_nmf = test_df['pred_nmf'].to_numpy()\n",
    "ground_truth_nmf = test_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:02<00:00, 600.37it/s]\n"
     ]
    }
   ],
   "source": [
    "######## CONTRACT LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "dataset = Dataset()\n",
    "user_ids = np.union1d(train_df['user'].unique(), test_df['user'].unique())\n",
    "item_ids = np.union1d(train_df['topic'].unique(), test_df['topic'].unique())\n",
    "dataset.fit(user_ids, item_ids)\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['topic'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "def topic_popular_contracts(df):\n",
    "    item_rating_sum = df.groupby(['topic', 'item'])['rating'].sum().reset_index()\n",
    "    sorted_items = item_rating_sum.sort_values(['topic', 'rating'], ascending=[True, False])\n",
    "    topic_to_popular_items = {k: g['item'].tolist() for k, g in sorted_items.groupby('topic')}\n",
    "    return topic_to_popular_items\n",
    "\n",
    "test_df['pred_cmf'] = 0\n",
    "topic_popular_contracts_dict = topic_popular_contracts(test_df)\n",
    "\n",
    "for user, user_data in tqdm(test_df.groupby('user'), total=test_df['user'].nunique()):\n",
    "    user_id_internal = user_ids_mapping[user]\n",
    "    item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['topic']])\n",
    "    predictions = model.predict(user_id_internal, item_ids_internal)\n",
    "    test_df.loc[user_data.index, 'pred_cmf'] = predictions\n",
    "\n",
    "pred_cmf = test_df['pred_cmf'].to_numpy()\n",
    "ground_truth_cmf = test_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:02<00:00, 643.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@1: 0.10093727469358327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:02<00:00, 609.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@5: 0.07238644556596972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### METRIC EVAL #######\n",
    "model_mode_eval = 'nmf'\n",
    "\n",
    "def precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_labels = ground_truth[top_k_indices]\n",
    "    hit = np.sum(top_k_labels > 0)\n",
    "    return hit / k\n",
    "\n",
    "def ap_at_k(k, ground_truth, pred, user_ids, edge_index):\n",
    "    precisions = []\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        \n",
    "        precisions.append(\n",
    "            precision_at_k(user_id, sorted_indices, filtered_ground_truth, k)\n",
    "        )\n",
    "        \n",
    "    return np.mean(precisions)\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute DCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - DCG@k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - NDCG@k\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "def calculate_ndcg_at_k(k, ground_truth, pred, edge_index):\n",
    "    \"\"\"\n",
    "    Calculate the average NDCG@k for all users\n",
    "    \n",
    "    Parameters:\n",
    "    - k: Rank\n",
    "    - ground_truth: True relevance scores\n",
    "    - pred: Predicted relevance scores\n",
    "    - edge_index: User-item interaction indices\n",
    "    \n",
    "    Returns:\n",
    "    - Average NDCG@k\n",
    "    \"\"\"\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "    ndcgs = []\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        \n",
    "        # Sort by predicted score\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        sorted_ground_truth = filtered_ground_truth[sorted_indices]\n",
    "        \n",
    "        ndcgs.append(ndcg_at_k(sorted_ground_truth, k))\n",
    "        \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "\n",
    "def evaluate(k_values, test_data_prime, ground_truth, pred):\n",
    "    edge_index = test_data_prime['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "\n",
    "    for k in k_values:\n",
    "        ### HIT@K ###\n",
    "        hit_at_k = ap_at_k(k, ground_truth, pred, user_ids, edge_index)\n",
    "        print(f\"AP@{k}: {hit_at_k}\")\n",
    "        # ### NDCG@K ###\n",
    "        # ndcg_result = calculate_ndcg_at_k(k, ground_truth, pred, edge_index)\n",
    "        # print(f\"NDCG@{k}: {ndcg_result}\")\n",
    "\n",
    "\n",
    "eval_loader = {\n",
    "    'gnn': {\n",
    "        'ground_truth': ground_truth_gnn,\n",
    "        'pred': pred_gnn\n",
    "    },\n",
    "    'nmf': {\n",
    "        'ground_truth': ground_truth_nmf,\n",
    "        'pred': pred_nmf\n",
    "    },\n",
    "    'cmf': {\n",
    "        'ground_truth': ground_truth_cmf,\n",
    "        'pred': pred_cmf\n",
    "    },\n",
    "\n",
    "}\n",
    "k_values = [1, 5]\n",
    "evaluate(k_values, test_data_all2all, ground_truth=eval_loader[model_mode_eval]['ground_truth'], pred=eval_loader[model_mode_eval]['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL V1 ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "            break\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
