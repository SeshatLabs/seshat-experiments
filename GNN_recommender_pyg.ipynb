{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSTALATION #######\n",
    "pip uninstall torch -y\n",
    "pip install torch==1.13.1\n",
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### IMPORT #######\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of item IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to items:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "def movie_loader():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "    extract_zip(download_url(url, '.'), '.')\n",
    "    movies_path = './ml-latest-small/movies.csv'\n",
    "    ratings_path = './ml-latest-small/ratings.csv'\n",
    "    items_ratings_df = pd.read_csv(ratings_path)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    items_df = pd.read_csv(movies_path)\n",
    "    items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "    items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "    items_df = items_df.sort_values('mappedID') # (Just the last 20 movies have NaN mappedId)\n",
    "    genres = items_df['genres'].str.get_dummies('|')\n",
    "    print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "    item_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "    assert item_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat\n",
    "\n",
    "def contract_loader():\n",
    "    items_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "    items_ratings_df = items_ratings_df[:100000]\n",
    "    items_df = {}\n",
    "    items_df['name'] = items_ratings_df['item'].unique()\n",
    "    items_df['itemId'], unique_names = pd.factorize(items_df['name'])\n",
    "    items_df['itemId'] = items_df['itemId'] + 1\n",
    "    items_df = pd.DataFrame(items_df, columns=['itemId', 'name'])\n",
    "\n",
    "    contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "    contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "    contracts_df_top_words = items_df.merge(contract_top_words_df, on='name', how='left')\n",
    "    contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "    items_df = contracts_df_top_words\n",
    "    items_df.set_index('itemId', inplace=True)\n",
    "    # f =5\n",
    "    items_df['truncated_keywords'] = items_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "    X_df = items_df['truncated_keywords'].str.get_dummies(',')\n",
    "    item_feat = torch.from_numpy(X_df.values).to(torch.float)\n",
    "    print(item_feat.shape)\n",
    "    print(item_feat)\n",
    "    print(items_df)\n",
    "    print(X_df)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat\n",
    "\n",
    "universal_mode = 'contract'\n",
    "loaders = {\n",
    "    'contract_loader': contract_loader,\n",
    "    'movie_loader': movie_loader,\n",
    "}\n",
    "unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat = loaders[f'{universal_mode}_loader']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LINK BINARY PRED MODEL ##########\n",
    "def train_test_generator(unique_user_id, items_df, item_feat, edge_index_user_to_item):  \n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0,\n",
    "        num_test=0.2,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return data, train_data, test_data\n",
    "\n",
    "def GNN_recommender(data, train_data):\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(item_feat.shape[1], hidden_channels)\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## AUC EVAL VALIDATION #########\n",
    "    # edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    # edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # # val_data has neg samples in it\n",
    "    # val_loader = LinkNeighborLoader(\n",
    "    #     data=val_data,\n",
    "    #     num_neighbors=[20, 10],\n",
    "    #     edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "    #     edge_label=edge_label,\n",
    "    #     batch_size=3 * 128,\n",
    "    #     shuffle=False,\n",
    "    # )\n",
    "    # sampled_data = next(iter(val_loader))\n",
    "    # preds = []\n",
    "    # ground_truths = []\n",
    "    # for sampled_data in tqdm(val_loader):\n",
    "    #     with torch.no_grad():\n",
    "    #         sampled_data.to(device)\n",
    "    #         preds.append(model(sampled_data))\n",
    "    #         ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    # pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    # ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    # auc = roc_auc_score(ground_truth, pred)\n",
    "    # print()\n",
    "    # print(f\"Validation AUC: {auc:.4f}\")\n",
    "    # return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TRAIN TEST GENERAION ############\n",
    "\n",
    "### ABLATION EXPRIMENT ### uncomment below for hiding item_features\n",
    "# item_feat = torch.zeros_like(item_feat)\n",
    "\n",
    "data, train_data, test_data = train_test_generator(unique_user_id, items_df, item_feat, edge_index_user_to_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test edges shape BEFORE adding all possible user item pairs torch.Size([2, 8000])\n",
      "test edges shape AFTER adding all possible user item pairs torch.Size([2, 17552028])\n",
      "unique users 4602\n",
      "unique items 3814\n"
     ]
    }
   ],
   "source": [
    "######## ALL_TO_ALL USER_ITEM PAIRS GENERATOR IN TEST_DATA #########\n",
    "\n",
    "# If mode GNN run below\n",
    "### SLICING TEST_DATA FOR ALL_TO_ALL EVAL ###\n",
    "slice_rate = 0.4\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label_index = test_data[\"user\", \"rates\", \"item\"].edge_label_index[:, : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0]))]\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label = test_data[\"user\", \"rates\", \"item\"].edge_label[ : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label))]\n",
    "\n",
    "edge_index_test = set(zip(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].numpy(), test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].numpy()))\n",
    "\n",
    "all_users = test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].unique().numpy()\n",
    "all_items = test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy()\n",
    "\n",
    "new_edges = []\n",
    "new_labels = []\n",
    "\n",
    "for user_id in all_users:\n",
    "    for item_id in all_items:\n",
    "        if (user_id, item_id) not in edge_index_test:\n",
    "            new_edges.append((user_id, item_id))\n",
    "            new_labels.append(0)\n",
    "\n",
    "import copy\n",
    "test_data_prime = copy.deepcopy(test_data)\n",
    "\n",
    "if new_edges:\n",
    "    new_edges_tensor = torch.tensor(new_edges, dtype=torch.int64).t().contiguous()\n",
    "    new_labels_tensor = torch.tensor(new_labels, dtype=torch.int64)\n",
    "\n",
    "    test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index = torch.cat((test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index, new_edges_tensor), dim=1)\n",
    "    test_data_prime[\"user\", \"rates\", \"item\"].edge_label = torch.cat((test_data_prime[\"user\", \"rates\", \"item\"].edge_label, new_labels_tensor), dim=0)\n",
    "\n",
    "print('test edges shape BEFORE adding all possible user item pairs', test_data[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "print('test edges shape AFTER adding all possible user item pairs', test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "\n",
    "print('unique users', len(test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index[0].unique()))\n",
    "print('unique items', len(test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index[1].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### EXPRIMENTS #######\n",
    "\n",
    "#TODO: Check how many users/items are not in train set, if it is small, what we should do?\n",
    "\n",
    "train_data_unique_users = set(train_data['user', 'rates', 'item'].edge_label_index[0].unique())\n",
    "train_data_unique_items = set(train_data['user', 'rates', 'item'].edge_label_index[1].unique())\n",
    "unique_users = set(unique_user_id['mappedID'].unique())\n",
    "unique_items = set(unique_item_id['mappedID'].unique())\n",
    "users_not_in_train = unique_users - train_data_unique_users\n",
    "items_not_in_train = unique_items - train_data_unique_items\n",
    "print(len(users_not_in_train))\n",
    "\n",
    "mask_user = torch.tensor([user in users_not_in_train for user in test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index[0]])\n",
    "mask_item = torch.tensor([item in items_not_in_train for item in test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index[1]])\n",
    "\n",
    "test_data_prime_ucsp = copy.deepcopy(test_data_prime)\n",
    "test_data_prime_icsp = copy.deepcopy(test_data_prime)\n",
    "test_data_prime_ucsp = test_data_prime_ucsp[\"user\", \"rates\", \"item\"].edge_label_index[:, mask_user]\n",
    "test_data_prime_ucsp = test_data_prime_ucsp[\"user\", \"rates\", \"item\"].edge_label[mask_user]\n",
    "\n",
    "test_data_prime_ucsp = test_data_prime_icsp[\"user\", \"rates\", \"item\"].edge_label_index[:, mask_item]\n",
    "test_data_prime_ucsp = test_data_prime_icsp[\"user\", \"rates\", \"item\"].edge_label[mask_item]\n",
    "\n",
    "# Now just need to pass each test_ucsp or test_icsp during PRED GEN time\n",
    "\n",
    "\n",
    "\n",
    "###### CONTRACTS MODEL ######\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model = GNN_recommender(unique_user_id, items_df, contract_feat, edge_index_user_to_contract)\n",
    "data, train_data, val_data, train_loader, test_data, model = GNN_recommender(unique_user_id, items_df, contract_feat, edge_index_user_to_contract)\n",
    "\n",
    "# Ablation study of GNN\n",
    "# ablation_without_contract_feature = torch.zeros_like(contract_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, items_df, ablation_without_contract_feature, edge_index_user_to_contract)\n",
    "\n",
    "\n",
    "\n",
    "# Cold Start User: Just need to change the test_df\n",
    "\n",
    "# Cold Start Item\n",
    "\n",
    "# Diversity\n",
    "\n",
    "# Contract Representation (effect of f top-keywords in contract_feat)\n",
    "\n",
    "# Sparsity (keep users just with > h interactions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:30<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.3302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.2516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.2255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## GNN TRAINING ############\n",
    "#if model_mode == GNN run below\n",
    "model = GNN_recommender(data, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45709/45709 [1:41:04<00:00,  7.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ground truth len 17552028\n"
     ]
    }
   ],
   "source": [
    "######## GNN PRED FOR TEST_DATA_PRIME #########\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data_prime,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"rates\", \"item\"), test_data_prime[\"user\", \"rates\", \"item\"].edge_label_index),\n",
    "    edge_label=test_data_prime[\"user\", \"rates\", \"item\"].edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "sampled_data = next(iter(test_loader))\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "pred_gnn = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth_gnn = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "print('all ground truth len', len(ground_truth_gnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR MF MODELS #############\n",
    "\n",
    "# if model_mode == 'MF':\n",
    "test_df_index = test_data_prime['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "test_df_label = test_data_prime['user', 'rates', 'item'].edge_label.numpy()\n",
    "test_df_index = test_df_index.T \n",
    "test_df = pd.DataFrame(test_df_index, columns=['user', 'item'])\n",
    "test_df['rating'] = test_df_label\n",
    "\n",
    "train_df_index = train_data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_df_label = train_data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_df_index = train_df_index.T \n",
    "train_df = pd.DataFrame(train_df_index, columns=['user', 'item'])\n",
    "train_df['rating'] = train_df_label\n",
    "\n",
    "\n",
    "contract_to_topic_df = pd.read_parquet(\"dataset/contract_name_topic.parquet\")\n",
    "\n",
    "# def add_topic(df):\n",
    "#     df['topic'] = ''\n",
    "#     for i, item in tqdm(df.iterrows(), total=len(df)):\n",
    "#         item_name = unique_item_id[unique_item_id['mappedID'] == item['item']]['itemId']\n",
    "#         topic = contract_to_topic_df[contract_to_topic_df['contract_name'] == item_name.item()]['most_probable_topic']\n",
    "        \n",
    "#         if topic.shape[0] == 0:  # No matches\n",
    "#             df.at[i, 'topic'] = 0\n",
    "#         elif topic.shape[0] > 1:  # Multiple matches\n",
    "#             df.at[i, 'topic'] = topic.iloc[0].item()\n",
    "#         else:  # Exactly one match\n",
    "#             df.at[i, 'topic'] = topic.item()\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# test_df = add_topic(test_df)\n",
    "# train_df = add_topic(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [00:31<00:00, 146.77it/s]\n"
     ]
    }
   ],
   "source": [
    "######## NAME LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "test_df['pred_nmf'] = 0\n",
    "\n",
    "for user, user_data in tqdm(test_df.groupby('user'), total=test_df['user'].nunique()):\n",
    "    user_id_internal = user_ids_mapping[user]\n",
    "    item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['item']])\n",
    "    predictions = model.predict(user_id_internal, item_ids_internal)\n",
    "    test_df.loc[user_data.index, 'pred_nmf'] = predictions\n",
    "\n",
    "pred_nmf = test_df['pred_nmf'].to_numpy()\n",
    "ground_truth_mf = test_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CONTRACT LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), np.arange(15)) # since we have 0 to 14 topics\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['topic'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "def topic_popular_contracts(df):\n",
    "    item_rating_sum = df.groupby(['topic', 'item'])['rating'].sum().reset_index()\n",
    "    sorted_items = item_rating_sum.sort_values(['topic', 'rating'], ascending=[True, False])\n",
    "    topic_to_popular_items = {k: g['item'].tolist() for k, g in sorted_items.groupby('topic')}\n",
    "    return topic_to_popular_items\n",
    "\n",
    "test_df['pred_cmf'] = 0\n",
    "topic_popular_contracts_dict = topic_popular_contracts(test_df)\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    user_id_internal = user_ids_mapping[row['user']]\n",
    "    # now topic_id is a prediction, we need to get the pred for all 14 topics, then sort it and return the topic (indices) with highest value\n",
    "    topic_pred = model.predict(user_id_internal, np.arange(15))\n",
    "    topic_id = topic_pred.argsort()[::-1][0] # we can get f predicted topic instead of the highest one\n",
    "    test_df['pred_cmf'][i] = topic_popular_contracts_dict[topic_id][0] # we are getting first popular contract in predicted topic\n",
    "\n",
    "\n",
    "pred_cmf = test_df['pred_cmf'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [05:22<00:00, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@1: 0.04758800521512386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [05:22<00:00, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@5: 0.03489787049109083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' NMF MovieLens\\nAP@1: 0.07635467980295567\\nAP@5: 0.06962233169129721\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### HIT@K EVAL V2 ##########\n",
    "#Note: One simple way for faster hit@k is to calculate the hit@k just for the first 1k users for all models\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    hit = 0\n",
    "    for i in sorted_indices[:k]:\n",
    "        if filtered_ground_truth[i] > 0: hit+= 1\n",
    "\n",
    "    return hit/k\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = test_data_prime['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = edge_index[0].unique()\n",
    "    count = 0\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth_mf, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth_mf, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth_gnn, pred_gnn, k))\n",
    "            count += 1\n",
    "            # if count % 100 == 0: \n",
    "            #     print(np.mean(precisions))\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='nmf')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL V1 ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "            break\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
