{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankhatiri/sean2/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting ./ml-latest-small.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Action  Adventure  Drama  Horror\n",
      "movieId                                  \n",
      "1             0          1      0       0\n",
      "2             0          1      0       0\n",
      "3             0          0      0       0\n",
      "4             0          0      1       0\n",
      "5             0          0      0       0\n",
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "   userId  mappedID\n",
      "0       1         0\n",
      "1       2         1\n",
      "2       3         2\n",
      "3       4         3\n",
      "4       5         4\n",
      "\n",
      "Mapping of item IDs to consecutive values:\n",
      "===========================================\n",
      "   itemId  mappedID\n",
      "0       1         0\n",
      "1       3         1\n",
      "2       6         2\n",
      "3      47         3\n",
      "4      50         4\n",
      "\n",
      "Final edge indices pointing from users to items:\n",
      "=================================================\n",
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    1,    2,  ..., 3121, 1392, 2873]])\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of item IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to items:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, edge_index_user_to_item\n",
    "\n",
    "# ##### MOVIELENS DATA ########\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "extract_zip(download_url(url, '.'), '.')\n",
    "movies_path = './ml-latest-small/movies.csv'\n",
    "ratings_path = './ml-latest-small/ratings.csv'\n",
    "movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
    "movies_df = movies_df.rename(columns={'movieId': 'itemId'})\n",
    "genres = movies_df['genres'].str.get_dummies('|')\n",
    "print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "# assert movie_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "movies_ratings_df = pd.read_csv(ratings_path)\n",
    "movies_ratings_df = movies_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "unique_user_id, edge_index_user_to_movie = data_loader(movies_ratings_df)\n",
    "\n",
    "###### CONTRACTS DATA ########\n",
    "# contracts_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "# # contracts_ratings_df = contracts_ratings_df[:10000]\n",
    "# contracts_df = {}\n",
    "# contracts_df['name'] = contracts_ratings_df['item'].unique()\n",
    "# contracts_df['itemId'], unique_names = pd.factorize(contracts_df['name'])\n",
    "# contracts_df['itemId'] = contracts_df['itemId'] + 1\n",
    "# contracts_df = pd.DataFrame(contracts_df, columns=['itemId', 'name'])\n",
    "\n",
    "# contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "# contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "# contracts_df_top_words = contracts_df.merge(contract_top_words_df, on='name', how='left')\n",
    "# contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "# contracts_df = contracts_df_top_words\n",
    "# contracts_df.set_index('itemId', inplace=True)\n",
    "# # f =5\n",
    "# contracts_df['truncated_keywords'] = contracts_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "# X_df = contracts_df['truncated_keywords'].str.get_dummies(',')\n",
    "# contract_feat = torch.tensor(X_df.values, dtype=torch.float)\n",
    "# print(contract_feat.shape[1])\n",
    "\n",
    "# contracts_ratings_df = contracts_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "# unique_user_id, edge_index_user_to_contract = data_loader(contracts_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PYG BINARY LINK PREDICTION ############\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def GNN_recommender(unique_user_id, items_df, item_feat, edge_index_user_to_item):\n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    # For this, we first split the set of edges into\n",
    "    # training (80%), validation (10%), and testing edges (10%).\n",
    "    # Across the training edges, we use 70% of edges for message passing,\n",
    "    # and 30% of edges for supervision.\n",
    "    # We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "    # Negative edges during training will be generated on-the-fly.\n",
    "    # We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=2.0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    # In the first hop, we sample at most 20 neighbors.\n",
    "    # In the second hop, we sample at most 10 neighbors.\n",
    "    # In addition, during training, we want to sample negative edges on-the-fly with\n",
    "    # a ratio of 2:1.\n",
    "    # We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "    from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    from torch_geometric.nn import SAGEConv, to_hetero\n",
    "    import torch.nn.functional as F\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "\n",
    "    class SimpleClassifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = torch.matmul(edge_feat_user, edge_feat_item.t())\n",
    "            return torch.sigmoid(scores)\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(20, hidden_channels) #put contract_feat.shape[1] for contracts\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "            # self.simple_classifier = SimpleClassifier()\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    model = Model(hidden_channels=64)\n",
    "\n",
    "    import tqdm\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 20):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    # Define the validation seed edges:\n",
    "    edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # val_data has neg samples in it\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=3 * 128,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    sampled_data = next(iter(val_loader))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for sampled_data in tqdm.tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    print()\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 58.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 48.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.3469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 52.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.3288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 52.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 60.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 55.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.2908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:04<00:00, 46.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 52.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 48.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.2592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 48.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.2522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 52.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 0.2448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 51.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 0.2411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 52.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.2324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:04<00:00, 47.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 47.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.2220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 51.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 0.2181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:04<00:00, 45.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 0.2148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 48.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:03<00:00, 47.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 0.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 113.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## EXPERIMENTS ############\n",
    "###### MOVIELENS MODEL ######\n",
    "data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, movies_df, movie_feat, edge_index_user_to_movie)\n",
    "\n",
    "###### CONTRACTS MODEL ######\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred = GNN_recommender(unique_user_id, contracts_df, contract_feat, edge_index_user_to_contract)\n",
    "\n",
    "# Ablation study of GNN (make contract_feat all zero)\n",
    "# INFUTURE_ add timestamp to ?, then remove timestamp, add social_graph (user-user txs)\n",
    "\n",
    "# ablation_without_contract_feature = torch.zeros_like(contract_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred = GNN_recommender(unique_user_id, contracts_df, ablation_without_contract_feature, edge_index_user_to_contract)\n",
    "\n",
    "# ablation_without_movie_feature = torch.zeros_like(movie_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred = GNN_recommender(unique_user_id, movies_df, movie_feat, edge_index_user_to_movie)\n",
    "\n",
    "# Cold Start User\n",
    "\n",
    "# Cold Start Item\n",
    "\n",
    "# Diversity\n",
    "\n",
    "# Contract Representation (effect of f top-keywords in contract_feat)\n",
    "\n",
    "# Sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user={ node_id=[610] },\n",
      "  item={\n",
      "    node_id=[9742],\n",
      "    x=[9742, 20],\n",
      "  },\n",
      "  (user, rates, item)={\n",
      "    edge_index=[2, 80670],\n",
      "    edge_label=[30249],\n",
      "    edge_label_index=[2, 30249],\n",
      "  },\n",
      "  (item, rev_rates, user)={ edge_index=[2, 80670] },\n",
      "  (user, rats, item)={},\n",
      "  (user, ratse, item)={}\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4060785/3375834210.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_loader_df['pred'][i] = model.predict(user_id_internal, [item_id_internal])\n"
     ]
    }
   ],
   "source": [
    "# Fucking lost the name-level train and pred\n",
    "print(val_loader.data)\n",
    "val_loader_df_index = val_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "val_loader_df_label = val_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "val_loader_df_index = val_loader_df_index.T \n",
    "val_loader_df = pd.DataFrame(val_loader_df_index, columns=['user', 'item'])\n",
    "val_loader_df['rating'] = val_loader_df_label\n",
    "\n",
    "train_loader_df_index = train_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_loader_df_label = train_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_loader_df_index = train_loader_df_index.T \n",
    "train_loader_df = pd.DataFrame(train_loader_df_index, columns=['user', 'item'])\n",
    "train_loader_df['rating'] = train_loader_df_label\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_loader_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "val_loader_df['pred'] = 0\n",
    "for i, row in val_loader_df.iterrows():\n",
    "    user_id_internal = user_ids_mapping[row['user']]\n",
    "    item_id_internal = item_ids_mapping[row['item']]\n",
    "    val_loader_df['pred'][i] = model.predict(user_id_internal, [item_id_internal])\n",
    "\n",
    "pred_mf = val_loader_df['pred'].to_numpy()\n",
    "ground_truth_mf = val_loader_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30249 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30249/30249 [00:05<00:00, 5304.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@1: 0.3861615260008595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30249/30249 [00:05<00:00, 5208.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@2: 0.3345399847928857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30249/30249 [00:05<00:00, 5107.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@3: 0.2895412520524095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30249/30249 [00:06<00:00, 5014.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@4: 0.24752884392872493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30249/30249 [00:06<00:00, 4697.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@5: 0.21366656748983437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    \n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'mf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_mf, k)) # ground_truth is the same for both GNN and mf\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='mf')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9006\n",
      "9737\n"
     ]
    }
   ],
   "source": [
    "print(len(set(val_data['user', 'rates', 'item'].edge_label_index.numpy()[0])))\n",
    "print(len(set(val_data['user', 'rates', 'item'].edge_label_index.numpy()[1])))\n",
    "print(len(movies_df['title'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
