{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankhatiri/sean2/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               name\n",
      "itemId                             \n",
      "1                       TetherToken\n",
      "2       TransparentUpgradeableProxy\n",
      "3                       BridgeToken\n",
      "4                             Token\n",
      "5                           Seaport\n",
      "...                             ...\n",
      "31523                          HDTC\n",
      "31524                      FreeShop\n",
      "31525                     FaceKnots\n",
      "31526                   Brainwashed\n",
      "31527                     EtherBall\n",
      "\n",
      "[31527 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of movie IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to movies:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, edge_index_user_to_item\n",
    "\n",
    "###### MOVIELENS DATA ########\n",
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# extract_zip(download_url(url, '.'), '.')\n",
    "# movies_path = './ml-latest-small/movies.csv'\n",
    "# ratings_path = './ml-latest-small/ratings.csv'\n",
    "# movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
    "# movies_df = movies_df.rename(columns={'movieId': 'itemId'})\n",
    "# genres = movies_df['genres'].str.get_dummies('|')\n",
    "# print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "# movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "# # assert movie_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "# movies_ratings_df = pd.read_csv(ratings_path)\n",
    "# movies_ratings_df = movies_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "# unique_user_id, edge_index_user_to_movie = data_loader(movies_ratings_df)\n",
    "\n",
    "###### CONTRACTS DATA ########\n",
    "contracts_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "contracts_df = {}\n",
    "contracts_df['name'] = contracts_ratings_df['item'].unique()\n",
    "contracts_df['itemId'], unique_names = pd.factorize(contracts_df['name'])\n",
    "contracts_df['itemId'] = contracts_df['itemId'] + 1\n",
    "contracts_df = pd.DataFrame(contracts_df, columns=['itemId', 'name'])\n",
    "contracts_df.set_index('itemId', inplace=True)\n",
    "\n",
    "\n",
    "# contracts_ratings_df = contracts_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "# unique_user_id, edge_index_user_to_contract = data_loader(contracts_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the top_k_words columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PYG BINARY LINK PREDICTION ############\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def GNN_recommender(unique_user_id, items_df, item_feat, edge_index_user_to_item):\n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    # For this, we first split the set of edges into\n",
    "    # training (80%), validation (10%), and testing edges (10%).\n",
    "    # Across the training edges, we use 70% of edges for message passing,\n",
    "    # and 30% of edges for supervision.\n",
    "    # We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "    # Negative edges during training will be generated on-the-fly.\n",
    "    # We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=2.0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    # In the first hop, we sample at most 20 neighbors.\n",
    "    # In the second hop, we sample at most 10 neighbors.\n",
    "    # In addition, during training, we want to sample negative edges on-the-fly with\n",
    "    # a ratio of 2:1.\n",
    "    # We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "    from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    from torch_geometric.nn import SAGEConv, to_hetero\n",
    "    import torch.nn.functional as F\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_user = x_user[edge_label_index[0]]\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            # Apply dot-product to get a prediction per supervision edge:\n",
    "            return (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "\n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(20, hidden_channels)\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    model = Model(hidden_channels=64)\n",
    "\n",
    "    import tqdm\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 6):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    # Define the validation seed edges:\n",
    "    edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # val_data has neg samples in it\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=3 * 128,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    sampled_data = next(iter(val_loader))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    #TODO: How to get the predictions in pred just for a user, then calculate avg_percision by knowing the ground_truth \n",
    "    # (how many of 1s are ranked in top-k)\n",
    "    for sampled_data in tqdm.tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    print()\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    return val_data, ground_truth, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             title   \n",
      "movieId                                              \n",
      "1                                 Toy Story (1995)  \\\n",
      "2                                   Jumanji (1995)   \n",
      "3                          Grumpier Old Men (1995)   \n",
      "4                         Waiting to Exhale (1995)   \n",
      "5               Father of the Bride Part II (1995)   \n",
      "...                                            ...   \n",
      "193581   Black Butler: Book of the Atlantic (2017)   \n",
      "193583                No Game No Life: Zero (2017)   \n",
      "193585                                Flint (2017)   \n",
      "193587         Bungo Stray Dogs: Dead Apple (2018)   \n",
      "193609         Andrew Dice Clay: Dice Rules (1991)   \n",
      "\n",
      "                                              genres  \n",
      "movieId                                               \n",
      "1        Adventure|Animation|Children|Comedy|Fantasy  \n",
      "2                         Adventure|Children|Fantasy  \n",
      "3                                     Comedy|Romance  \n",
      "4                               Comedy|Drama|Romance  \n",
      "5                                             Comedy  \n",
      "...                                              ...  \n",
      "193581               Action|Animation|Comedy|Fantasy  \n",
      "193583                      Animation|Comedy|Fantasy  \n",
      "193585                                         Drama  \n",
      "193587                              Action|Animation  \n",
      "193609                                        Comedy  \n",
      "\n",
      "[9742 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MOVIELENS MODEL ######\n",
    "val_data, ground_truth, pred = GNN_recommender(unique_user_id, movies_df, movie_feat, edge_index_user_to_movie)\n",
    "\n",
    "###### CONTRACTS MODEL ######\n",
    "val_data, ground_truth, pred = GNN_recommender(unique_user_id, contracts_df, contract_feat, edge_index_user_to_contarct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6997057753975339\n"
     ]
    }
   ],
   "source": [
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "\n",
    "def calculate_ap_at_k(user_id, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_edge_index = edge_index[:, mask]\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    \n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "k = 5\n",
    "precisions = []\n",
    "edge_index = val_data['user', 'rates', 'item'].edge_label_index\n",
    "for user_id in edge_index[0]:\n",
    "    precisions.append(calculate_ap_at_k(user_id, ground_truth, pred, k))\n",
    "\n",
    "hit_at_k = np.mean(precisions)\n",
    "print(hit_at_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data has around 30k edge label, and 30k edge index tuples (from, to). 10k of them are positive edges and others are neg samples\n",
    "\n",
    "print(val_data['user', 'rates', 'movie'])\n",
    "\n",
    "# print(pred[:5])\n",
    "# print(ground_truth[:5])\n",
    "# print(len(pred))\n",
    "# print(len(ground_truth))\n",
    "# print(val_data['user', 'rates', 'movie'].edge_index)\n",
    "# count = 0\n",
    "# for sampled_data in val_loader:\n",
    "#     count += 1\n",
    "#     if count % 4 == 0 : \n",
    "#         print(sampled_data)\n",
    "#         # print(sampled_data['movie'])\n",
    "#         # print(sampled_data[\"user\", \"rates\", \"movie\"].edge_label)\n",
    "#         # print(sampled_data[\"user\", \"rates\", \"movie\"].edge_label_index)\n",
    "#         break\n",
    "# print(val_data)\n",
    "# print(genres.values[:5])\n",
    "# print(len(ratings_user_id))\n",
    "# print(ratings_df[:5])\n",
    "# ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "#                             left_on='userId', right_on='userId', how='left')\n",
    "# print(ratings_user_id[:5])\n",
    "# ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "# print(ratings_user_id[:5])\n",
    "\n",
    "# print(train_loader.data)\n",
    "# print(data[\"user\", \"rates\", \"movie\"].edge_index)\n",
    "\n",
    "# print(len(val_data[\"user\", \"rates\", \"movie\"].edge_label))\n",
    "# print(val_data[\"user\", \"rates\", \"movie\"].edge_label_index)\n",
    "# print(val_data[\"user\", \"rates\", \"movie\"].edge_label_index[0][15122:15128])\n",
    "# val_data[\"user\", \"rates\", \"movie\"].edge_label_index[1][15122:15128]\n",
    "\n",
    "# val_data[\"user\", \"rates\", \"movie\"].edge_label[11000:11050]\n",
    "\n",
    "# From Val_data just group_by user_id, this will be ground truth, add X random neg samples from items where edge_label=0\n",
    "# concatenate two list, load this data with LinkNeighborLoader, pass it to model and return the preds\n",
    "user_id = 11\n",
    "\n",
    "edge_index = val_data['user', 'rates', 'movie']['edge_label_index']\n",
    "edge_label = val_data['user', 'rates', 'movie']['edge_label']\n",
    "\n",
    "mask = edge_index[0] == user_id\n",
    "filtered_edge_index = edge_index[:, mask]\n",
    "filtered_edge_label = edge_label[mask]\n",
    "\n",
    "# To create the val_ap_data we need to pass the movie_feat, node_ids. the movie_id in movie_feat is equal to movie_node_ids\n",
    "val_ap_data = HeteroData()\n",
    "val_ap_data[\"user\"].node_id = filtered_edge_index[0]\n",
    "val_ap_data[\"movie\"].node_id = filtered_edge_index[1]\n",
    "# Add the node features and edge indices:\n",
    "\n",
    "val_ap_data[\"movie\"].x = movie_feat[??, :]\n",
    "val_ap_data[\"user\", \"rates\", \"movie\"].edge_index = edge_index_user_to_movie\n",
    "# We also need to make sure to add the reverse edges from movies to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "val_ap_data = T.ToUndirected()(val_ap_data)\n",
    "\n",
    "\n",
    "# val_ap_loader = LinkNeighborLoader(\n",
    "#     data=val_data,\n",
    "#     num_neighbors=[20, 10],\n",
    "#     edge_label_index=filtered_edge_index,\n",
    "#     edge_label=filtered_edge_label,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "# sampled_data = next(iter(val_ap_loader))\n",
    "\n",
    "# preds = []\n",
    "# ground_truths = []\n",
    "# for sampled_data in tqdm.tqdm(val_loader):\n",
    "#     with torch.no_grad():\n",
    "#         sampled_data.to(device)\n",
    "#         preds.append(model(sampled_data))\n",
    "#         ground_truths.append(sampled_data[\"user\", \"rates\", \"movie\"].edge_label)\n",
    "# pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "# ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
