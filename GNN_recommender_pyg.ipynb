{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4343, 4338])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]])\n",
      "                               name   \n",
      "itemId                                \n",
      "1                       TetherToken  \\\n",
      "2       TransparentUpgradeableProxy   \n",
      "3                       BridgeToken   \n",
      "4                             Token   \n",
      "5                           Seaport   \n",
      "...                             ...   \n",
      "4339                   GenesisVault   \n",
      "4340                        LetsApe   \n",
      "4341                        Isoroom   \n",
      "4342                     AstroGator   \n",
      "4343                  ZooFrenzToken   \n",
      "\n",
      "                                                 keywords   \n",
      "itemId                                                      \n",
      "1       dev, erc20, upgraded, address, oncode, title, ...  \\\n",
      "2       proxy, admin, proxyadmin, implementation, dedi...   \n",
      "3       beacon, _implementation_slot, erc1967, newimpl...   \n",
      "4       dev, token, ierc20, supply, erc20, allowance, ...   \n",
      "5       conduit, consideration, orders, order, 0age, i...   \n",
      "...                                                   ...   \n",
      "4339    00, 000, 0000, 000000000, 000000000000000000, ...   \n",
      "4340    00, 000, 0000, 000000000, 000000000000000000, ...   \n",
      "4341    room, integrity, publish, prove, genesis, seed...   \n",
      "4342    eips, dev, basic, fungible, erc721, title, tok...   \n",
      "4343    00, 000, 0000, 000000000, 000000000000000000, ...   \n",
      "\n",
      "                                       truncated_keywords  \n",
      "itemId                                                     \n",
      "1       dev, erc20, upgraded, address, oncode, title, ...  \n",
      "2       proxy, admin, proxyadmin, implementation, dedi...  \n",
      "3       beacon, _implementation_slot, erc1967, newimpl...  \n",
      "4       dev, token, ierc20, supply, erc20, allowance, ...  \n",
      "5       conduit, consideration, orders, order, 0age, i...  \n",
      "...                                                   ...  \n",
      "4339    00, 000, 0000, 000000000, 000000000000000000, ...  \n",
      "4340    00, 000, 0000, 000000000, 000000000000000000, ...  \n",
      "4341    room, integrity, publish, prove, genesis, seed...  \n",
      "4342    eips, dev, basic, fungible, erc721, title, tok...  \n",
      "4343    00, 000, 0000, 000000000, 000000000000000000, ...  \n",
      "\n",
      "[4343 rows x 3 columns]\n",
      "         00   000   0000   000000000   000000000000000000   \n",
      "itemId                                                      \n",
      "1         0     0      0           0                    0  \\\n",
      "2         0     0      0           0                    0   \n",
      "3         0     0      0           0                    0   \n",
      "4         0     0      0           0                    0   \n",
      "5         0     0      0           0                    0   \n",
      "...     ...   ...    ...         ...                  ...   \n",
      "4339      0     1      1           1                    1   \n",
      "4340      0     1      1           1                    1   \n",
      "4341      0     0      0           0                    0   \n",
      "4342      0     0      0           0                    0   \n",
      "4343      0     1      1           1                    1   \n",
      "\n",
      "         000000000000000000000000000000000000000000000000000000000004cce0   \n",
      "itemId                                                                      \n",
      "1                                                       0                  \\\n",
      "2                                                       0                   \n",
      "3                                                       0                   \n",
      "4                                                       0                   \n",
      "5                                                       0                   \n",
      "...                                                   ...                   \n",
      "4339                                                    1                   \n",
      "4340                                                    1                   \n",
      "4341                                                    0                   \n",
      "4342                                                    0                   \n",
      "4343                                                    1                   \n",
      "\n",
      "         000000004294967296   0001   000281474976710656   00100000  ...  ymm   \n",
      "itemId                                                              ...        \n",
      "1                         0      0                    0          0  ...    0  \\\n",
      "2                         0      0                    0          0  ...    0   \n",
      "3                         0      0                    0          0  ...    0   \n",
      "4                         0      0                    0          0  ...    0   \n",
      "5                         0      0                    0          0  ...    0   \n",
      "...                     ...    ...                  ...        ...  ...  ...   \n",
      "4339                      1      1                    1          1  ...    0   \n",
      "4340                      1      1                    1          1  ...    0   \n",
      "4341                      0      0                    0          0  ...    0   \n",
      "4342                      0      0                    0          0  ...    0   \n",
      "4343                      1      1                    1          1  ...    0   \n",
      "\n",
      "        zeppelin  zerion  zero  zombie  zoo  zora  zx  контракта  ꁒꍟ  \n",
      "itemId                                                                \n",
      "1              0       0     0       0    0     0   0          0   0  \n",
      "2              0       0     0       0    0     0   0          0   0  \n",
      "3              0       0     0       0    0     0   0          0   0  \n",
      "4              0       0     0       0    0     0   0          0   0  \n",
      "5              0       0     0       0    0     0   0          0   0  \n",
      "...          ...     ...   ...     ...  ...   ...  ..        ...  ..  \n",
      "4339           0       0     0       0    0     0   0          0   0  \n",
      "4340           0       0     0       0    0     0   0          0   0  \n",
      "4341           0       0     0       0    0     0   0          0   0  \n",
      "4342           0       0     0       0    0     0   0          0   0  \n",
      "4343           0       0     0       0    0     0   0          0   0  \n",
      "\n",
      "[4343 rows x 4338 columns]\n",
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "                                       userId  mappedID\n",
      "0  0xc23d11e5ae70a680f6f3e278503007dbae4b868c         0\n",
      "1  0x365782e192de5abdd3183da25d2e94a2641a616d         1\n",
      "2  0xe03795684f8be6c958ea8c389be07ecd4eb89848         2\n",
      "3  0x3811b30aff6af58314bb8864d0f455a4089c6331         3\n",
      "4  0xb6a990a19b69ba982f0b4a1bebbce212e337da9f         4\n",
      "\n",
      "Mapping of item IDs to consecutive values:\n",
      "===========================================\n",
      "                        itemId  mappedID\n",
      "0                  TetherToken         0\n",
      "1  TransparentUpgradeableProxy         1\n",
      "2                  BridgeToken         2\n",
      "3                        Token         3\n",
      "4                      Seaport         4\n",
      "\n",
      "Final edge indices pointing from users to items:\n",
      "=================================================\n",
      "tensor([[   0,    1,    1,  ..., 1139, 1139, 1139],\n",
      "        [   0,    1,    2,  ..., 4342,  129, 4308]])\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of item IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to items:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "# ##### MOVIELENS DATA ########\n",
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# extract_zip(download_url(url, '.'), '.')\n",
    "# movies_path = './ml-latest-small/movies.csv'\n",
    "# ratings_path = './ml-latest-small/ratings.csv'\n",
    "# items_ratings_df = pd.read_csv(ratings_path)\n",
    "# items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "# unique_user_id, unique_item_id, edge_index_user_to_movie = data_loader(items_ratings_df)\n",
    "# items_df = pd.read_csv(movies_path)\n",
    "# items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "# items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "# items_df = items_df.sort_values('mappedID') # (Just the last 20 movies have NaN mappedId)\n",
    "# genres = items_df['genres'].str.get_dummies('|')\n",
    "# print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "# movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "# # assert movie_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "\n",
    "###### CONTRACTS DATA ########\n",
    "items_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "items_ratings_df = items_ratings_df[:10000]\n",
    "items_df = {}\n",
    "items_df['name'] = items_ratings_df['item'].unique()\n",
    "items_df['itemId'], unique_names = pd.factorize(items_df['name'])\n",
    "items_df['itemId'] = items_df['itemId'] + 1\n",
    "items_df = pd.DataFrame(items_df, columns=['itemId', 'name'])\n",
    "\n",
    "contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "contracts_df_top_words = items_df.merge(contract_top_words_df, on='name', how='left')\n",
    "contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "items_df = contracts_df_top_words\n",
    "items_df.set_index('itemId', inplace=True)\n",
    "# f =5\n",
    "items_df['truncated_keywords'] = items_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "X_df = items_df['truncated_keywords'].str.get_dummies(',')\n",
    "contract_feat = torch.from_numpy(X_df.values).to(torch.float)\n",
    "print(contract_feat.shape)\n",
    "print(contract_feat)\n",
    "print(items_df)\n",
    "print(X_df)\n",
    "items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "unique_user_id, unique_item_id, edge_index_user_to_contract = data_loader(items_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PYG BINARY LINK PREDICTION ############\n",
    "\n",
    "def GNN_recommender(unique_user_id, items_df, item_feat, edge_index_user_to_item):\n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    # For this, we first split the set of edges into\n",
    "    # training (80%), validation (10%), and testing edges (10%).\n",
    "    # Across the training edges, we use 70% of edges for message passing,\n",
    "    # and 30% of edges for supervision.\n",
    "    # We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "    # Negative edges during training will be generated on-the-fly.\n",
    "    # We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=2.0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    # In the first hop, we sample at most 20 neighbors.\n",
    "    # In the second hop, we sample at most 10 neighbors.\n",
    "    # In addition, during training, we want to sample negative edges on-the-fly with\n",
    "    # a ratio of 2:1.\n",
    "    # We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(contract_feat.shape[1], hidden_channels) #put contract_feat.shape[1] for contracts\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## EVAL VALIDATION #########\n",
    "    edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # val_data has neg samples in it\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=3 * 128,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    sampled_data = next(iter(val_loader))\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for sampled_data in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    print()\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SIMPLE PYG BINARY LINK PREDICTION ############\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# INPROGRESS\n",
    "#TODO: split the edge_index, then for each set create the x and fed along edge_index{set_nme}\n",
    "x_user = torch.randn(len(unique_user_id), 20)  # node features\n",
    "x_item = movie_feat\n",
    "print(movie_feat)\n",
    "x = torch.cat((x_user, x_item), dim=0)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index_user_to_movie)\n",
    "print(data)\n",
    "train_length = int(0.8 * len(data))\n",
    "test_length = len(data) - train_length\n",
    "\n",
    "train_data, test_data = random_split(data, [train_length, test_length])\n",
    "print(train_data)\n",
    "\n",
    "\n",
    "unique_users = set()\n",
    "unique_items = set()\n",
    "\n",
    "for data in train_loader:\n",
    "    user, item = data.edge_index\n",
    "    unique_users.update(user.tolist())\n",
    "    unique_items.update(item.tolist())\n",
    "\n",
    "train_target = torch.zeros(len(unique_users), len(unique_items), dtype=torch.float32)\n",
    "\n",
    "for data in train_loader:\n",
    "    user, item = data.edge_index  # Assuming each data point is a tuple of (user, item)\n",
    "    train_target[user, item] = 1.0\n",
    "\n",
    "\n",
    "# class GNNRec(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(GNNRec, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, 64)\n",
    "#         self.conv2 = GCNConv(64, out_channels)\n",
    "    \n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# model = GNNRec(16, 32)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Function to compute the prediction scores using dot product\n",
    "# def compute_scores(user_embeddings, movie_embeddings):\n",
    "#     scores = torch.matmul(user_embeddings, movie_embeddings.t())\n",
    "#     return torch.sigmoid(scores)\n",
    "\n",
    "# # Sample training loop\n",
    "# model.train()\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)\n",
    "#     user_embeddings, movie_embeddings = out[:len(unique_user_id)], out[len(unique_user_id):] # this should be len(users) and len(movies)\n",
    "#     scores = compute_scores(user_embeddings, movie_embeddings)\n",
    "#     # Sample loss: You would ideally want pairs with interactions to have scores close to 1 and others close to 0\n",
    "#     target = torch.tensor([[1, 0, 0], \n",
    "#                            [0, 1, 1],\n",
    "#                            [0, 0, 1]], dtype=torch.float) # This is a placeholder. Replace with your actual targets.\n",
    "#     loss = F.binary_cross_entropy(scores, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# # Predictions\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     out = model(data)\n",
    "#     user_embeddings, movie_embeddings = out[:3], out[3:]\n",
    "#     predictions = compute_scores(user_embeddings, movie_embeddings)\n",
    "    \n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 19.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 19.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 19.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 20.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 20.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## EXPERIMENTS ############\n",
    "###### MOVIELENS MODEL ######\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, items_df, movie_feat, edge_index_user_to_movie)\n",
    "\n",
    "# ablation_without_movie_feature = torch.zeros_like(movie_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, items_df, ablation_without_movie_feature, edge_index_user_to_movie)\n",
    "\n",
    "###### CONTRACTS MODEL ######\n",
    "data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model = GNN_recommender(unique_user_id, items_df, contract_feat, edge_index_user_to_contract)\n",
    "\n",
    "# Ablation study of GNN\n",
    "# ablation_without_contract_feature = torch.zeros_like(contract_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, items_df, ablation_without_contract_feature, edge_index_user_to_contract)\n",
    "\n",
    "\n",
    "\n",
    "# Cold Start User: Just need to change the test_df\n",
    "\n",
    "# Cold Start Item\n",
    "\n",
    "# Diversity\n",
    "\n",
    "# Contract Representation (effect of f top-keywords in contract_feat)\n",
    "\n",
    "# Sparsity (keep users just with > h interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user={ node_id=[1140] },\n",
      "  item={\n",
      "    node_id=[4343],\n",
      "    x=[4343, 4338],\n",
      "  },\n",
      "  (user, rates, item)={\n",
      "    edge_index=[2, 5600],\n",
      "    edge_label=[2400],\n",
      "    edge_label_index=[2, 2400],\n",
      "  },\n",
      "  (item, rev_rates, user)={ edge_index=[2, 5600] }\n",
      ")\n",
      "HeteroData(\n",
      "  user={ node_id=[1140] },\n",
      "  item={\n",
      "    node_id=[4343],\n",
      "    x=[4343, 4338],\n",
      "  },\n",
      "  (user, rates, item)={ edge_index=[2, 10000] },\n",
      "  (item, rev_rates, user)={ edge_index=[2, 10000] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# How to pick a slice from val_data? This will yield the preds in less time since we will have less unique_user and items\n",
    "# print(val_data)\n",
    "print(train_data)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5488/5488 [01:51<00:00, 49.09it/s]\n"
     ]
    }
   ],
   "source": [
    "######## GNN PRED FOR ALL USER ITEM PAIRS IN VAL_LOADER #########\n",
    "# BUG: now we are adding the (user,item) pair for all users and items in data, but it should be just for test_data, maybe we can regenerate the test_data with neg_sample=0\n",
    "edge_index_val = set(zip(val_data[\"user\", \"rates\", \"item\"].edge_label_index[0].numpy(), val_data[\"user\", \"rates\", \"item\"].edge_label_index[1].numpy()))\n",
    "\n",
    "all_users = val_data[\"user\", \"rates\", \"item\"].edge_label_index[0].unique().numpy()\n",
    "all_items = val_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy()\n",
    "\n",
    "new_edges = []\n",
    "new_labels = []\n",
    "\n",
    "for user_id in all_users:\n",
    "    for item_id in all_items:\n",
    "        if (user_id, item_id) not in edge_index_val:\n",
    "            new_edges.append((user_id, item_id))\n",
    "            new_labels.append(0)\n",
    "\n",
    "# Convert lists to tensors and concatenate them to the original tensors\n",
    "if new_edges:\n",
    "    new_edges_tensor = torch.tensor(new_edges, dtype=torch.int64).t().contiguous()\n",
    "    new_labels_tensor = torch.tensor(new_labels, dtype=torch.int64)\n",
    "\n",
    "    val_data[\"user\", \"rates\", \"item\"].edge_label_index = torch.cat((val_data[\"user\", \"rates\", \"item\"].edge_label_index, new_edges_tensor), dim=1)\n",
    "    val_data[\"user\", \"rates\", \"item\"].edge_label = torch.cat((val_data[\"user\", \"rates\", \"item\"].edge_label, new_labels_tensor), dim=0)\n",
    "\n",
    "#TODO: The way that we calculating the hit@k dosen't make sense, what about given the positive edges in val_set, group by user, get the prediction list for user and all items\n",
    "# sort the preds, now we cna have hit@k and NDCG and very other metrics. If it takes too long, simply get a slice of val_data\n",
    "\n",
    "# val_data_pos = val_loader_df[val_loader_df['rating'] == 1]\n",
    "# data = HeteroData()\n",
    "# data[\"user\"].node_id = torch.from_numpy(val_data_pos['user'].unique()).to(torch.int64)\n",
    "# data[\"item\"].node_id = torch.from_numpy(val_data_pos['item'].unique()).to(torch.int64)\n",
    "# #TODO Is this contract_feat correct, or we should fed all the contratcs_feat instead of selecting festures just for unqiue contracts in this small set?\n",
    "# data[\"item\"].x = contract_feat[torch.from_numpy(val_data_pos['item'].unique()).to(torch.long)] \n",
    "# # data[\"item\"].x = contract_feat\n",
    "# # for each user in unique_users, get the top-k item predicted by model\n",
    "# ratings_user_id = torch.empty(0, dtype=torch.int64)\n",
    "# ratings_item_id = torch.empty(0, dtype=torch.int64)\n",
    "# for user in val_data_pos['user'].unique():\n",
    "#     ratings_user_id = torch.cat((ratings_user_id, torch.full((len(val_data_pos['item'].unique()), ), user, dtype=torch.int64)), dim=0)\n",
    "#     ratings_item_id = torch.cat((ratings_item_id, data[\"item\"].node_id), dim=0)\n",
    "\n",
    "# data[\"user\", \"rates\", \"item\"].edge_index = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "# temp_ground_truth = []\n",
    "# for user, item in zip(data[\"user\", \"rates\", \"item\"].edge_index.numpy()[0], data[\"user\", \"rates\", \"item\"].edge_index.numpy()[1]):\n",
    "#     if val_data_pos[(val_data_pos['user'] == user) & (val_data_pos['item'] == item)].shape[0] > 0:\n",
    "#         temp_ground_truth.append(1)\n",
    "#     else:\n",
    "#         temp_ground_truth.append(0)\n",
    "# data[\"user\", \"rates\", \"item\"].edge_label = torch.tensor(temp_ground_truth).to(torch.int64)\n",
    "# data = T.ToUndirected()(data)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "val_loader_temp = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"rates\", \"item\"), val_data[\"user\", \"rates\", \"item\"].edge_label_index),\n",
    "    edge_label=val_data[\"user\", \"rates\", \"item\"].edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "sampled_data = next(iter(val_loader_temp))\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm(val_loader_temp):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "print(len(ground_truth))\n",
    "print(len(val_data[\"user\", \"rates\", \"item\"].edge_label_index[0].unique()))\n",
    "print(len(val_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR MF MODELS #############\n",
    "print(val_loader.data)\n",
    "val_loader_df_index = val_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "val_loader_df_label = val_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "val_loader_df_index = val_loader_df_index.T \n",
    "val_loader_df = pd.DataFrame(val_loader_df_index, columns=['user', 'item'])\n",
    "val_loader_df['rating'] = val_loader_df_label\n",
    "\n",
    "train_loader_df_index = train_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_loader_df_label = train_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_loader_df_index = train_loader_df_index.T \n",
    "train_loader_df = pd.DataFrame(train_loader_df_index, columns=['user', 'item'])\n",
    "train_loader_df['rating'] = train_loader_df_label\n",
    "\n",
    "\n",
    "contract_to_topic_df = pd.read_parquet(\"dataset/contract_name_topic.parquet\")\n",
    "\n",
    "def add_topic(df):\n",
    "    df['topic'] = ''\n",
    "    for i, item in df.iterrows():\n",
    "        item_name = unique_item_id[unique_item_id['mappedID'] == item['item']]['itemId']\n",
    "        topic = contract_to_topic_df[contract_to_topic_df['contract_name'] == item_name.item()]['most_probable_topic']\n",
    "        \n",
    "        if topic.shape[0] == 0:  # No matches\n",
    "            df.at[i, 'topic'] = 0\n",
    "        elif topic.shape[0] > 1:  # Multiple matches\n",
    "            df.at[i, 'topic'] = topic.iloc[0].item()  # or handle it differently\n",
    "        else:  # Exactly one match\n",
    "            df.at[i, 'topic'] = topic.item()\n",
    "    \n",
    "    return df\n",
    "\n",
    "val_loader_df = add_topic(val_loader_df)\n",
    "train_loader_df = add_topic(train_loader_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NAME LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_loader_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "val_loader_df['pred_nmf'] = 0\n",
    "for i, row in val_loader_df.iterrows():\n",
    "    user_id_internal = user_ids_mapping[row['user']]\n",
    "    item_id_internal = item_ids_mapping[row['item']]\n",
    "    val_loader_df['pred_nmf'][i] = model.predict(user_id_internal, [item_id_internal])\n",
    "\n",
    "pred_nmf = val_loader_df['pred_nmf'].to_numpy()\n",
    "ground_truth = val_loader_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 35, 50, 53, 68, 80, 20, 28, 29, 36, 38, 41, 65, 95]\n"
     ]
    }
   ],
   "source": [
    "######## CONTRACT LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), np.arange(15)) # since we have 0 to 14 topics\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['topic'], row['rating']) for index, row in train_loader_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "def topic_popular_contracts(df):\n",
    "    item_rating_sum = df.groupby(['topic', 'item'])['rating'].sum().reset_index()\n",
    "    sorted_items = item_rating_sum.sort_values(['topic', 'rating'], ascending=[True, False])\n",
    "    topic_to_popular_items = {k: g['item'].tolist() for k, g in sorted_items.groupby('topic')}\n",
    "    return topic_to_popular_items\n",
    "\n",
    "val_loader_df['pred_cmf'] = 0\n",
    "topic_popular_contracts_dict = topic_popular_contracts(val_loader_df)\n",
    "for i, row in val_loader_df.iterrows():\n",
    "    user_id_internal = user_ids_mapping[row['user']]\n",
    "    # now topic_id is a prediction, we need to get the pred for all 14 topics, then sort it and return the topic (indices) with highest value\n",
    "    topic_pred = model.predict(user_id_internal, np.arange(15))\n",
    "    topic_id = topic_pred.argsort()[::-1][0] # we can get f predicted topic instead of the highest one\n",
    "    val_loader_df['pred_cmf'][i] = topic_popular_contracts_dict[topic_id] # we are getting all popular contracts in predicted topic\n",
    "\n",
    "\n",
    "pred_cmf = val_loader_df['pred_cmf'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL V2 ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    print(filtered_pred)\n",
    "    print(len(filtered_pred))\n",
    "    print(filtered_ground_truth)\n",
    "    print(len(filtered_ground_truth))\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "            break\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL V1 ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "            break\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
