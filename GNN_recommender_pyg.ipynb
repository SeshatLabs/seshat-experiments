{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch-scatter -y\n",
    "!pip uninstall torch-sparse -y\n",
    "!pip uninstall pyg-lib -y\n",
    "!pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "   userId  mappedID\n",
      "0       1         0\n",
      "1       2         1\n",
      "2       3         2\n",
      "3       4         3\n",
      "4       5         4\n",
      "\n",
      "Mapping of item IDs to consecutive values:\n",
      "===========================================\n",
      "   itemId  mappedID\n",
      "0       1         0\n",
      "1       3         1\n",
      "2       6         2\n",
      "3      47         3\n",
      "4      50         4\n",
      "\n",
      "Final edge indices pointing from users to items:\n",
      "=================================================\n",
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    1,    2,  ..., 3121, 1392, 2873]])\n",
      "    Action  Adventure  Drama  Horror\n",
      "0        0          1      0       0\n",
      "2        0          0      0       0\n",
      "5        1          0      0       0\n",
      "43       0          0      0       0\n",
      "46       0          0      0       0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting ./ml-latest-small.zip\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    # Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    print(\"Mapping of user IDs to consecutive values:\")\n",
    "    print(\"==========================================\")\n",
    "    print(unique_user_id.head())\n",
    "    print()\n",
    "    # Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    print(\"Mapping of item IDs to consecutive values:\")\n",
    "    print(\"===========================================\")\n",
    "    print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    # With this, we are ready to construct our `edge_index` in COO format\n",
    "    # following PyG semantics:\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # assert edge_index_user_to_item.size() == (2, 100836)\n",
    "    print()\n",
    "    print(\"Final edge indices pointing from users to items:\")\n",
    "    print(\"=================================================\")\n",
    "    print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "# ##### MOVIELENS DATA ########\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "extract_zip(download_url(url, '.'), '.')\n",
    "movies_path = './ml-latest-small/movies.csv'\n",
    "ratings_path = './ml-latest-small/ratings.csv'\n",
    "movies_ratings_df = pd.read_csv(ratings_path)\n",
    "movies_ratings_df = movies_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "unique_user_id, unique_item_id, edge_index_user_to_movie = data_loader(movies_ratings_df)\n",
    "movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
    "movies_df = movies_df.rename(columns={'movieId': 'itemId'})\n",
    "movies_df = pd.merge(movies_df, unique_item_id, left_on='movieId', right_on='itemId', how='left')\n",
    "movies_df = movies_df.sort_values('mappedID')\n",
    "genres = movies_df['genres'].str.get_dummies('|')\n",
    "print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "# assert movie_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "\n",
    "###### CONTRACTS DATA ########\n",
    "# contracts_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "# # contracts_ratings_df = contracts_ratings_df[:10000]\n",
    "# contracts_df = {}\n",
    "# contracts_df['name'] = contracts_ratings_df['item'].unique()\n",
    "# contracts_df['itemId'], unique_names = pd.factorize(contracts_df['name'])\n",
    "# contracts_df['itemId'] = contracts_df['itemId'] + 1\n",
    "# contracts_df = pd.DataFrame(contracts_df, columns=['itemId', 'name'])\n",
    "\n",
    "# contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "# contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "# contracts_df_top_words = contracts_df.merge(contract_top_words_df, on='name', how='left')\n",
    "# contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "# contracts_df = contracts_df_top_words\n",
    "# contracts_df.set_index('itemId', inplace=True)\n",
    "# # f =5\n",
    "# contracts_df['truncated_keywords'] = contracts_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "# X_df = contracts_df['truncated_keywords'].str.get_dummies(',')\n",
    "# contract_feat = torch.tensor(X_df.values, dtype=torch.float)\n",
    "# print(contract_feat.shape[1])\n",
    "\n",
    "# contracts_ratings_df = contracts_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "# unique_user_id, edge_index_user_to_contract = data_loader(contracts_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PYG BINARY LINK PREDICTION ############\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def GNN_recommender(unique_user_id, items_df, item_feat, edge_index_user_to_item):\n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(len(items_df))\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    # For this, we first split the set of edges into\n",
    "    # training (80%), validation (10%), and testing edges (10%).\n",
    "    # Across the training edges, we use 70% of edges for message passing,\n",
    "    # and 30% of edges for supervision.\n",
    "    # We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "    # Negative edges during training will be generated on-the-fly.\n",
    "    # We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=2.0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    # In the first hop, we sample at most 20 neighbors.\n",
    "    # In the second hop, we sample at most 10 neighbors.\n",
    "    # In addition, during training, we want to sample negative edges on-the-fly with\n",
    "    # a ratio of 2:1.\n",
    "    # We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "    from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    from torch_geometric.nn import SAGEConv, to_hetero\n",
    "    import torch.nn.functional as F\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "\n",
    "    class SimpleClassifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = torch.matmul(edge_feat_user, edge_feat_item.t())\n",
    "            return torch.sigmoid(scores)\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(20, hidden_channels) #put contract_feat.shape[1] for contracts\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "            # self.simple_classifier = SimpleClassifier()\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    import tqdm\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## EVAL VALIDATION #########\n",
    "    edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # val_data has neg samples in it\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=3 * 128,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    sampled_data = next(iter(val_loader))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    for sampled_data in tqdm.tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            preds.append(model(sampled_data))\n",
    "            ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    print()\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0.]])\n",
      "                           title                                       genres\n",
      "movieId                                                                      \n",
      "1               Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
      "2                 Jumanji (1995)                   Adventure|Children|Fantasy\n",
      "3        Grumpier Old Men (1995)                               Comedy|Romance\n"
     ]
    }
   ],
   "source": [
    "print(movie_feat[:3]) # it has movieId\n",
    "print(movies_df[:3]) # it has movieId\n",
    "# print(movies_ratings_df[movies_ratings_df['userId'] == 1]) # it has movieId\n",
    "# print(edge_index_user_to_movie) # it has movies mappedId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## SIMPLE PYG BINARY LINK PREDICTION ############\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#TODO: split the edge_index, then for each set create the x and fed along edge_index{set_nme}\n",
    "x_user = torch.randn(len(unique_user_id), 20)  # node features\n",
    "x_item = movie_feat\n",
    "print(movie_feat)\n",
    "x = torch.cat((x_user, x_item), dim=0)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index_user_to_movie)\n",
    "print(data)\n",
    "train_length = int(0.8 * len(data))\n",
    "test_length = len(data) - train_length\n",
    "\n",
    "train_data, test_data = random_split(data, [train_length, test_length])\n",
    "print(train_data)\n",
    "\n",
    "\n",
    "unique_users = set()\n",
    "unique_items = set()\n",
    "\n",
    "for data in train_loader:\n",
    "    user, item = data.edge_index\n",
    "    unique_users.update(user.tolist())\n",
    "    unique_items.update(item.tolist())\n",
    "\n",
    "train_target = torch.zeros(len(unique_users), len(unique_items), dtype=torch.float32)\n",
    "\n",
    "for data in train_loader:\n",
    "    user, item = data.edge_index  # Assuming each data point is a tuple of (user, item)\n",
    "    train_target[user, item] = 1.0\n",
    "\n",
    "\n",
    "# class GNNRec(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(GNNRec, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, 64)\n",
    "#         self.conv2 = GCNConv(64, out_channels)\n",
    "    \n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# model = GNNRec(16, 32)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Function to compute the prediction scores using dot product\n",
    "# def compute_scores(user_embeddings, movie_embeddings):\n",
    "#     scores = torch.matmul(user_embeddings, movie_embeddings.t())\n",
    "#     return torch.sigmoid(scores)\n",
    "\n",
    "# # Sample training loop\n",
    "# model.train()\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)\n",
    "#     user_embeddings, movie_embeddings = out[:len(unique_user_id)], out[len(unique_user_id):] # this should be len(users) and len(movies)\n",
    "#     scores = compute_scores(user_embeddings, movie_embeddings)\n",
    "#     # Sample loss: You would ideally want pairs with interactions to have scores close to 1 and others close to 0\n",
    "#     target = torch.tensor([[1, 0, 0], \n",
    "#                            [0, 1, 1],\n",
    "#                            [0, 0, 1]], dtype=torch.float) # This is a placeholder. Replace with your actual targets.\n",
    "#     loss = F.binary_cross_entropy(scores, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# # Predictions\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     out = model(data)\n",
    "#     user_embeddings, movie_embeddings = out[:3], out[3:]\n",
    "#     predictions = compute_scores(user_embeddings, movie_embeddings)\n",
    "    \n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## EXPERIMENTS ############\n",
    "###### MOVIELENS MODEL ######\n",
    "data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, movies_df, movie_feat, edge_index_user_to_movie)\n",
    "\n",
    "# ablation_without_movie_feature = torch.zeros_like(movie_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, movies_df, ablation_without_movie_feature, edge_index_user_to_movie)\n",
    "\n",
    "###### CONTRACTS MODEL ######\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data = GNN_recommender(unique_user_id, contracts_df, contract_feat, edge_index_user_to_contract)\n",
    "\n",
    "# Ablation study of GNN\n",
    "# ablation_without_contract_feature = torch.zeros_like(contract_feat)\n",
    "# data, train_data, val_data, train_loader, val_loader, ground_truth, pred = GNN_recommender(unique_user_id, contracts_df, ablation_without_contract_feature, edge_index_user_to_contract)\n",
    "\n",
    "\n",
    "\n",
    "# Cold Start User\n",
    "\n",
    "# Cold Start Item\n",
    "\n",
    "# Diversity\n",
    "\n",
    "# Contract Representation (effect of f top-keywords in contract_feat)\n",
    "\n",
    "# Sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NAME LEVEL MF TRAIN & PRED #########\n",
    "print(val_loader.data)\n",
    "val_loader_df_index = val_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "val_loader_df_label = val_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "val_loader_df_index = val_loader_df_index.T \n",
    "val_loader_df = pd.DataFrame(val_loader_df_index, columns=['user', 'item'])\n",
    "val_loader_df['rating'] = val_loader_df_label\n",
    "\n",
    "train_loader_df_index = train_loader.data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_loader_df_label = train_loader.data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_loader_df_index = train_loader_df_index.T \n",
    "train_loader_df = pd.DataFrame(train_loader_df_index, columns=['user', 'item'])\n",
    "train_loader_df['rating'] = train_loader_df_label\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_loader_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "val_loader_df['pred'] = 0\n",
    "for i, row in val_loader_df.iterrows():\n",
    "    user_id_internal = user_ids_mapping[row['user']]\n",
    "    item_id_internal = item_ids_mapping[row['item']]\n",
    "    val_loader_df['pred'][i] = model.predict(user_id_internal, [item_id_internal])\n",
    "\n",
    "pred_mf = val_loader_df['pred'].to_numpy()\n",
    "ground_truth_mf = val_loader_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    \n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'mf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_mf, k)) # ground_truth is the same for both GNN and mf\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
