{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSTALATION #######\n",
    "\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1\n",
    "# !pip uninstall torch-scatter -y\n",
    "# !pip uninstall torch-sparse -y\n",
    "# !pip uninstall pyg-lib -y\n",
    "# !pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y\n",
    "# !pip uninstall sentence_transformers -y\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pyarrow fastparquet\n",
    "!pip install transformers\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install sentence_transformers==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### IMPORT #######\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_experiments = ['none', 'diversity', 'ucsp', 'icsp', 'usparcity', 'isparcity', 'sBERT', 'TFIDF', 'ablation_item_feat', 'ablation_social_edges']\n",
    "possible_experiments = {\n",
    "    0: 'none',\n",
    "    1: 'diversity',\n",
    "    2: 'ucsp',\n",
    "    3: 'icsp',\n",
    "    4: 'usparcity',\n",
    "    5: 'isparcity',\n",
    "    6: 'sBERT',\n",
    "    7: 'TFIDF',\n",
    "    8: 'ablation_item_feat', \n",
    "    9: 'ablation_social_edges', # meaning adding social relationships\n",
    "}\n",
    "possible_modes = ['debug', 'experiment']\n",
    "model_variants = ['gnn', 'nmf', 'cmf']\n",
    "\n",
    "experiment = possible_experiments[9]\n",
    "mode = possible_modes[0]\n",
    "model_variant_eval = model_variants[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item feature tensor shape torch.Size([17310, 8023])\n",
      "number of unique users 10761\n",
      "number of unique items 17310\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "\n",
    "def data_loader(ratings_df):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "    # print(\"Mapping of user IDs to consecutive values:\")\n",
    "    # print(\"==========================================\")\n",
    "    # print(unique_user_id.head())\n",
    "\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "    # print(\"Mapping of item IDs to consecutive values:\")\n",
    "    # print(\"===========================================\")\n",
    "    # print(unique_item_id.head())\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "    # print()\n",
    "    # print(\"Final edge indices pointing from users to items:\")\n",
    "    # print(\"=================================================\")\n",
    "    # print(edge_index_user_to_item)\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "def movie_loader():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "    extract_zip(download_url(url, '.'), '.')\n",
    "    movies_path = './ml-latest-small/movies.csv'\n",
    "    ratings_path = './ml-latest-small/ratings.csv'\n",
    "    items_ratings_df = pd.read_csv(ratings_path)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'movieId': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    items_df = pd.read_csv(movies_path)\n",
    "    items_df = items_df.rename(columns={'movieId': 'itemId', 'title': 'name'})\n",
    "    items_df = pd.merge(items_df, unique_item_id, on='itemId', how='left')\n",
    "    items_df = items_df.sort_values('mappedID') # (Just the last 20 movies have NaN mappedId)\n",
    "    genres = items_df['genres'].str.get_dummies('|')\n",
    "    print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "    item_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "    assert item_feat.size() == (9742, 20)  # 20 genres in total.\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df\n",
    "\n",
    "def contract_loader():\n",
    "    items_ratings_df = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "    def calculate_sparcity_value(df):\n",
    "        num_users = df['user'].nunique()\n",
    "        num_items = df['item'].nunique()\n",
    "        num_interactions = len(df)\n",
    "        total_possible_interactions = num_users * num_items / 100\n",
    "        sparsity = 1 - (num_interactions / total_possible_interactions)\n",
    "        return sparsity\n",
    "    \n",
    "    def filter_interactions(df, column, k):\n",
    "        valid_entries = df[column].value_counts()\n",
    "        valid_entries = valid_entries[valid_entries > k]\n",
    "        df = df[df[column].isin(valid_entries.index)]\n",
    "        print(f'{column} sparcity value is:', calculate_sparcity_value(df))\n",
    "        return df\n",
    "\n",
    "    ########## SPARCITY EXPERIMENT ###########\n",
    "    if experiment == 'usparcity':\n",
    "        k = 5\n",
    "        items_ratings_df = filter_interactions(items_ratings_df, 'user', k)\n",
    "    elif experiment == 'isparcity':\n",
    "        k = 5\n",
    "        items_ratings_df = filter_interactions(items_ratings_df, 'item', k)\n",
    "\n",
    "    items_ratings_df = items_ratings_df[:100000] if mode == 'debug' else items_ratings_df\n",
    "    items_df = {}\n",
    "    items_df['name'] = items_ratings_df['item'].unique()\n",
    "    items_df['itemId'], unique_names = pd.factorize(items_df['name'])\n",
    "    # items_df['itemId'] = items_df['itemId'] + 1 #TODO test commenting this line didn't breal anything\n",
    "    items_df = pd.DataFrame(items_df, columns=['itemId', 'name'])\n",
    "\n",
    "    def get_item_feat_sbert(items_df):\n",
    "        contract2comments = pd.read_parquet('dataset/contracts2comment.parquet')\n",
    "        c2c_main_class = contract2comments[contract2comments['contract_name'] == contract2comments['class_name']]\n",
    "\n",
    "        def reorder_text(text):\n",
    "            lines = text.split(\"\\n\")\n",
    "            notice_lines = [line for line in lines if \"@notice\" in line]\n",
    "            other_lines = [line for line in lines if \"@notice\" not in line]\n",
    "            reorderd_text = \"\\n\".join(notice_lines + other_lines)\n",
    "            return reorderd_text\n",
    "\n",
    "        def preprocess_text(text):\n",
    "            text = reorder_text(text)\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            # Remove special characters, numbers, etc.\n",
    "            text = re.sub(r'\\W', ' ', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            text = text[:512] if len(text) > 512 else text\n",
    "            return text\n",
    "\n",
    "        sentences = []\n",
    "        for i, item in items_df.iterrows():\n",
    "            comment_class = c2c_main_class[c2c_main_class['contract_name'] == item['name']]\n",
    "            if not comment_class.empty and comment_class['class_documentation'].iloc[0] != '':\n",
    "                sentences.append(comment_class['class_documentation'].iloc[0])\n",
    "            else:\n",
    "                class_names = contract2comments[contract2comments['contract_name'] == item['name']]['class_name']\n",
    "                sentences.append(' '.join(class_names))\n",
    "\n",
    "        preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else \"cpu\") # NOT enough GPU memory\n",
    "        model = model.to(device)\n",
    "        inputs = tokenizer(preprocessed_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        item_feat = embeddings\n",
    "        # model = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
    "        # embeddings = model.encode(preprocessed_sentences)\n",
    "        \n",
    "        return item_feat\n",
    "    \n",
    "    def get_item_feat_tfidf(items_df):\n",
    "        contract_top_words_df = pd.read_parquet('dataset/contract_top_words.parquet')\n",
    "        contract_top_words_df = contract_top_words_df.rename(columns={'contract_name': 'name'})\n",
    "        contracts_df_top_words = items_df.merge(contract_top_words_df, on='name', how='left')\n",
    "        contracts_df_top_words['keywords'] = contracts_df_top_words['keywords'].fillna('')\n",
    "        items_df = contracts_df_top_words\n",
    "        items_df.set_index('itemId', inplace=True)\n",
    "        # f =5 # ratio to determine the number of top keywords selected for each contract to construct item_feat\n",
    "        items_df['truncated_keywords'] = items_df['keywords'].apply(lambda x: ','.join(x.split(',')))\n",
    "        X_df = items_df['truncated_keywords'].str.get_dummies(',')\n",
    "        item_feat = torch.from_numpy(X_df.values).to(torch.float)\n",
    "        return item_feat\n",
    "    \n",
    "    ########### SBERT EXPERIMENT ###########\n",
    "    if experiment == 'sBERT':\n",
    "        item_feat = get_item_feat_sbert(items_df)\n",
    "    else:\n",
    "        item_feat = get_item_feat_tfidf(items_df)\n",
    "\n",
    "    print('item feature tensor shape', item_feat.shape)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    print('number of unique users', len(unique_user_id))\n",
    "    print('number of unique items', len(unique_item_id))\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df\n",
    "\n",
    "universal_mode = 'contract'\n",
    "loaders = {\n",
    "    'contract_loader': contract_loader,\n",
    "    'movie_loader': movie_loader,\n",
    "}\n",
    "unique_user_id, unique_item_id, edge_index_user_to_item, items_df, item_feat, items_ratings_df = loaders[f'{universal_mode}_loader']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LINK BINARY PRED MODEL ##########\n",
    "def train_test_generator(unique_user_id, item_feat, edge_index_user_to_item):  \n",
    "    data = HeteroData()\n",
    "    data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "    data[\"item\"].node_id = torch.arange(item_feat.shape[0])\n",
    "    data[\"item\"].x = item_feat\n",
    "    data[\"user\", \"rates\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data = T.ToUndirected()(data)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0,\n",
    "        num_test=0.2,\n",
    "        disjoint_train_ratio=0.3,\n",
    "        neg_sampling_ratio=0,\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=(\"user\", \"rates\", \"item\"),\n",
    "        rev_edge_types=(\"item\", \"rev_rates\", \"user\"), \n",
    "    )\n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return data, train_data, test_data\n",
    "\n",
    "def GNN_recommender(data, train_data):\n",
    "\n",
    "    # Define seed edges:\n",
    "    edge_label_index = train_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    edge_label = train_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=2.0,\n",
    "        edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    print('train loader:', type(train_loader))\n",
    "\n",
    "    class GNN(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    # Our final classifier applies the dot-product between source and destination\n",
    "    # node embeddings to derive edge-level predictions:\n",
    "    class Classifier(torch.nn.Module):\n",
    "        def forward(self, x_user: Tensor, x_item: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "            edge_feat_user = x_user[edge_label_index[0]] # Convert node embeddings to edge-level representations:\n",
    "            edge_feat_item = x_item[edge_label_index[1]]\n",
    "            scores = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "            return scores # Apply dot-product to get a prediction per supervision edge:\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            # Since the dataset does not come with rich features, we also learn two\n",
    "            # embedding matrices for users and items:\n",
    "            self.item_lin = torch.nn.Linear(item_feat.shape[1], hidden_channels)\n",
    "            self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "            self.item_emb = torch.nn.Embedding(data[\"item\"].num_nodes, hidden_channels)\n",
    "            # Instantiate homogeneous GNN:\n",
    "            self.gnn = GNN(hidden_channels)\n",
    "            # Convert GNN model into a heterogeneous variant:\n",
    "            self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "            self.classifier = Classifier()\n",
    "\n",
    "        def forward(self, data: HeteroData) -> Tensor:\n",
    "            x_dict = {\n",
    "            \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "            \"item\": self.item_lin(data[\"item\"].x) + self.item_emb(data[\"item\"].node_id),\n",
    "            } \n",
    "            # `x_dict` holds feature matrices of all node types\n",
    "            # `edge_index_dict` holds all edge indices of all edge types\n",
    "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "            pred = self.classifier(\n",
    "                x_dict[\"user\"],\n",
    "                x_dict[\"item\"],\n",
    "                data[\"user\", \"rates\", \"item\"].edge_label_index,\n",
    "            )\n",
    "            return pred\n",
    "            \n",
    "    ########## TRAINING ##########\n",
    "    model = Model(hidden_channels=64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: '{device}'\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10):\n",
    "        total_loss = total_examples = 0\n",
    "        for sampled_data in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data)\n",
    "            ground_truth = sampled_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "\n",
    "        # TODO: Add the val_loader, keep the best model\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n",
    "\n",
    "    ########## AUC EVAL VALIDATION #########\n",
    "    # edge_label_index = val_data[\"user\", \"rates\", \"item\"].edge_label_index\n",
    "    # edge_label = val_data[\"user\", \"rates\", \"item\"].edge_label\n",
    "    # # val_data has neg samples in it\n",
    "    # val_loader = LinkNeighborLoader(\n",
    "    #     data=val_data,\n",
    "    #     num_neighbors=[20, 10],\n",
    "    #     edge_label_index=((\"user\", \"rates\", \"item\"), edge_label_index),\n",
    "    #     edge_label=edge_label,\n",
    "    #     batch_size=3 * 128,\n",
    "    #     shuffle=False,\n",
    "    # )\n",
    "    # sampled_data = next(iter(val_loader))\n",
    "    # preds = []\n",
    "    # ground_truths = []\n",
    "    # for sampled_data in tqdm(val_loader):\n",
    "    #     with torch.no_grad():\n",
    "    #         sampled_data.to(device)\n",
    "    #         preds.append(model(sampled_data))\n",
    "    #         ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "    # pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    # ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    # auc = roc_auc_score(ground_truth, pred)\n",
    "    # print()\n",
    "    # print(f\"Validation AUC: {auc:.4f}\")\n",
    "    # return data, train_data, val_data, train_loader, val_loader, ground_truth, pred, test_data, model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge index shape before adding social edges: torch.Size([2, 100000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1738651/1738651 [09:05<00:00, 3188.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge index shape after adding social edges: torch.Size([2, 107559])\n"
     ]
    }
   ],
   "source": [
    "########## TRAIN TEST GENERAION ############\n",
    "\n",
    "####### ITEM FEAT ABLATION EXPRIMENT ####### \n",
    "if experiment == 'ablation_item_feat':\n",
    "    item_feat = torch.zeros_like(item_feat)\n",
    "\n",
    "# ####### SOCIAL EDGES ABLEATION EXPERIMENT #######\n",
    "def add_social_edges(edge_index_user_to_item, unique_item_id, items_ratings_df, item_feat):\n",
    "    user_transactions_df = pd.read_csv('dataset/user_transactions.csv')\n",
    "    contract_addresses = pd.read_csv('dataset/contract_addresses.csv')\n",
    "    contract_set = set(contract_addresses['address'])\n",
    "    \n",
    "    edge_index_user_to_item[1] = edge_index_user_to_item[1] + len(edge_index_user_to_item[0].unique())\n",
    "    unique_item_id['mappedID'] = unique_item_id['mappedID'] + len(edge_index_user_to_item[0].unique())\n",
    "    user_feat = torch.zeros((len(edge_index_user_to_item[0].unique()), item_feat.shape[1]))\n",
    "    item_feat= torch.cat([item_feat, user_feat], dim=0)\n",
    "\n",
    "    unique_user_id['type'] = 'user'\n",
    "    unique_item_id['type'] = 'item'\n",
    "    node2id = pd.concat([\n",
    "        unique_item_id.rename(columns={'itemId': 'entityId'}),\n",
    "        unique_user_id.rename(columns={'userId': 'entityId'})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    users = items_ratings_df['userId'].unique()\n",
    "\n",
    "    print('edge index shape before adding social edges:', edge_index_user_to_item.shape)\n",
    "    for i, interaction in tqdm(user_transactions_df.iterrows(), total=len(user_transactions_df)):\n",
    "        if interaction['from'] not in contract_set and interaction['to'] not in contract_set and interaction['from'] in users and  interaction['to'] in users:\n",
    "            from_user_id = node2id[node2id['entityId'] == interaction['from']]['mappedID'].iloc[0]\n",
    "            to_user_id = node2id[node2id['entityId'] == interaction['to']]['mappedID'].iloc[0]\n",
    "            social_edge = torch.tensor([[from_user_id], \n",
    "                                        [to_user_id]], dtype=torch.int64)\n",
    "            edge_index_user_to_item = torch.cat([edge_index_user_to_item, social_edge], dim=1)\n",
    "    print('edge index shape after adding social edges:', edge_index_user_to_item.shape)\n",
    "    \n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, item_feat\n",
    "if experiment == 'ablation_social_edges':\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item, item_feat = add_social_edges(edge_index_user_to_item, unique_item_id, items_ratings_df, item_feat)\n",
    "\n",
    "\n",
    "data, train_data, test_data = train_test_generator(unique_user_id, item_feat, edge_index_user_to_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CSP EXPRIMENTS #######\n",
    "### CSP #### note: if the ratio==1, rerun from the first step\n",
    "if experiment == 'ucsp' or experiment == 'icsp':\n",
    "    def csp_test_gen(train_data, test_data, unique_data, entity_index, experiment_abbr):\n",
    "        train_data_unique_entities = set(train_data['user', 'rates', 'item'].edge_label_index[entity_index].unique().numpy())\n",
    "        unique_entities = set(unique_data['mappedID'].unique())\n",
    "        entities_not_in_train = unique_entities - train_data_unique_entities\n",
    "        mask = torch.tensor([entity in entities_not_in_train for entity in test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index].numpy()])\n",
    "        \n",
    "        test_data_filtered = copy.deepcopy(test_data)\n",
    "        test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[:, mask]\n",
    "        test_data_filtered[\"user\", \"rates\", \"item\"].edge_label = test_data_filtered[\"user\", \"rates\", \"item\"].edge_label[mask]\n",
    "        \n",
    "        ratio = len(test_data_filtered[\"user\", \"rates\", \"item\"].edge_label_index[entity_index]) / len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[entity_index])\n",
    "        print(f'test to train ratio {experiment_abbr}', ratio)\n",
    "        \n",
    "        return test_data_filtered, ratio\n",
    "\n",
    "    test_data_csp, test_to_train_ratio_csp = csp_test_gen(\n",
    "        train_data, test_data, unique_user_id, 0 if experiment == 'ucsp' else 1, 'CSP-user' if experiment == 'ucsp' else 'CSP-item'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test edges shape BEFORE adding all possible user item pairs torch.Size([2, 2000])\n",
      "test edges shape AFTER adding all possible user item pairs torch.Size([2, 2108459])\n",
      "unique test users 1699\n",
      "unique test items 1241\n"
     ]
    }
   ],
   "source": [
    "######## ALL_TO_ALL USER_ITEM PAIRS GENERATOR IN TEST_DATA #########\n",
    "\n",
    "# If mode GNN run below\n",
    "### SLICING TEST_DATA FOR ALL_TO_ALL EVAL ###\n",
    "slice_rate = 0.1\n",
    "if experiment == 'ucsp' or experiment == 'icsp': \n",
    "    slice_rate = 1\n",
    "    test_data = test_data_csp\n",
    "\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label_index = test_data[\"user\", \"rates\", \"item\"].edge_label_index[:, : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0]))]\n",
    "test_data[\"user\", \"rates\", \"item\"].edge_label = test_data[\"user\", \"rates\", \"item\"].edge_label[ : int(slice_rate * len(test_data[\"user\", \"rates\", \"item\"].edge_label))]\n",
    "\n",
    "edge_index_test = set(zip(test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].numpy(), test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].numpy()))\n",
    "\n",
    "all_users = test_data[\"user\", \"rates\", \"item\"].edge_label_index[0].unique().numpy()\n",
    "all_items = test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy()\n",
    "\n",
    "# which elp the model most: keep the social_edges in test and be evaluated or remove all social_edges in test_set?\n",
    "if experiment == 'ablation_social_edges':\n",
    "    print(len(all_items))\n",
    "    all_items = [item for item in test_data[\"user\", \"rates\", \"item\"].edge_label_index[1].unique().numpy() if item > len(all_users)]\n",
    "    print(len(all_items))\n",
    "\n",
    "new_edges = []\n",
    "new_labels = []\n",
    "\n",
    "#TODO instead of all possible pairs, we can continue for each user if it reaches to x samples (pos + neg)\n",
    "for user_id in all_users:\n",
    "    for item_id in all_items:\n",
    "        if (user_id, item_id) not in edge_index_test:\n",
    "            new_edges.append((user_id, item_id))\n",
    "            new_labels.append(0)\n",
    "\n",
    "import copy\n",
    "test_data_all2all = copy.deepcopy(test_data)\n",
    "\n",
    "if new_edges:\n",
    "    new_edges_tensor = torch.tensor(new_edges, dtype=torch.int64).t().contiguous()\n",
    "    new_labels_tensor = torch.tensor(new_labels, dtype=torch.int64)\n",
    "\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index, new_edges_tensor), dim=1)\n",
    "    test_data_all2all[\"user\", \"rates\", \"item\"].edge_label = torch.cat((test_data_all2all[\"user\", \"rates\", \"item\"].edge_label, new_labels_tensor), dim=0)\n",
    "\n",
    "print('test edges shape BEFORE adding all possible user item pairs', test_data[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "print('test edges shape AFTER adding all possible user item pairs', test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index.shape)\n",
    "\n",
    "print('unique test users', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[0].unique()))\n",
    "print('unique test items', len(test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index[1].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader: <class 'torch_geometric.loader.link_neighbor_loader.LinkNeighborLoader'>\n",
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:18<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:18<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.2838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:17<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:19<00:00,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.1599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## GNN TRAINING ############\n",
    "#if model_mode == GNN run below\n",
    "model = GNN_recommender(data, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5491/5491 [08:39<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ground truth len 2108459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######## GNN PRED FOR TEST_DATA_PRIME ######### for CSP chnage the TEST_DATA_PRIME\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data=test_data_all2all,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"rates\", \"item\"), test_data_all2all[\"user\", \"rates\", \"item\"].edge_label_index),\n",
    "    edge_label=test_data_all2all[\"user\", \"rates\", \"item\"].edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "sampled_data = next(iter(test_loader))\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"user\", \"rates\", \"item\"].edge_label)\n",
    "pred_gnn = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth_gnn = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "print('all ground truth len', len(ground_truth_gnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR MF MODELS #############\n",
    "\n",
    "# if model_mode == 'MF':\n",
    "test_df_index = test_data_all2all['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "test_df_label = test_data_all2all['user', 'rates', 'item'].edge_label.numpy()\n",
    "test_df_index = test_df_index.T \n",
    "test_df = pd.DataFrame(test_df_index, columns=['user', 'item'])\n",
    "test_df['rating'] = test_df_label\n",
    "\n",
    "train_df_index = train_data['user', 'rates', 'item'].edge_label_index.numpy()\n",
    "train_df_label = train_data['user', 'rates', 'item'].edge_label.numpy()\n",
    "train_df_index = train_df_index.T \n",
    "train_df = pd.DataFrame(train_df_index, columns=['user', 'item'])\n",
    "train_df['rating'] = train_df_label\n",
    "\n",
    "def add_topic(df, contract_to_topic_df, unique_item_id):\n",
    "    item_to_topic = pd.Series(contract_to_topic_df['most_probable_topic'].values, index=contract_to_topic_df['contract_name']).to_dict()\n",
    "    mappedID_to_itemId = pd.Series(unique_item_id['itemId'].values, index=unique_item_id['mappedID']).to_dict()\n",
    "    df['item_name'] = df['item'].map(mappedID_to_itemId)\n",
    "    df['topic'] = df['item_name'].map(item_to_topic).fillna(0).astype(int)\n",
    "    df = df.drop(columns=['item_name'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "contract_to_topic_df = pd.read_parquet(\"dataset/contract_name_topic.parquet\")\n",
    "test_df = add_topic(test_df, contract_to_topic_df, unique_item_id)\n",
    "train_df = add_topic(train_df, contract_to_topic_df, unique_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NAME LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "# dataset.fit(data['user'].node_id.numpy(), data['item'].node_id.numpy())\n",
    "user_ids = np.union1d(train_df['user'].unique(), test_df['user'].unique())\n",
    "item_ids = np.union1d(train_df['item'].unique(), test_df['item'].unique())\n",
    "dataset.fit(user_ids, item_ids)\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['item'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "test_df['pred_nmf'] = 0\n",
    "\n",
    "for user, user_data in tqdm(test_df.groupby('user'), total=test_df['user'].nunique()):\n",
    "    user_id_internal = user_ids_mapping[user]\n",
    "    item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['item']])\n",
    "    predictions = model.predict(user_id_internal, item_ids_internal)\n",
    "    test_df.loc[user_data.index, 'pred_nmf'] = predictions\n",
    "\n",
    "pred_nmf = test_df['pred_nmf'].to_numpy()\n",
    "ground_truth_nmf = test_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CONTRACT LEVEL MF TRAIN & PRED #########\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "dataset = Dataset()\n",
    "user_ids = np.union1d(train_df['user'].unique(), test_df['user'].unique())\n",
    "item_ids = np.union1d(train_df['topic'].unique(), test_df['topic'].unique())\n",
    "dataset.fit(user_ids, item_ids)\n",
    "user_ids_mapping, _, item_ids_mapping, _ = dataset.mapping()\n",
    "\n",
    "(train_interactions, train_interactions_weight) = dataset.build_interactions((row['user'], row['topic'], row['rating']) for index, row in train_df.iterrows())\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(train_interactions, epochs=30, num_threads=2, sample_weight=train_interactions_weight)\n",
    "\n",
    "def topic_popular_contracts(df):\n",
    "    item_rating_sum = df.groupby(['topic', 'item'])['rating'].sum().reset_index()\n",
    "    sorted_items = item_rating_sum.sort_values(['topic', 'rating'], ascending=[True, False])\n",
    "    topic_to_popular_items = {k: g['item'].tolist() for k, g in sorted_items.groupby('topic')}\n",
    "    return topic_to_popular_items\n",
    "\n",
    "test_df['pred_cmf'] = 0\n",
    "topic_popular_contracts_dict = topic_popular_contracts(test_df)\n",
    "\n",
    "for user, user_data in tqdm(test_df.groupby('user'), total=test_df['user'].nunique()):\n",
    "    user_id_internal = user_ids_mapping[user]\n",
    "    item_ids_internal = np.array([item_ids_mapping[item] for item in user_data['topic']])\n",
    "    predictions = model.predict(user_id_internal, item_ids_internal)\n",
    "    test_df.loc[user_data.index, 'pred_cmf'] = predictions\n",
    "\n",
    "pred_cmf = test_df['pred_cmf'].to_numpy()\n",
    "ground_truth_cmf = test_df['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:06<00:00, 280.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT@1: 0.03825779870512066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:06<00:00, 268.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT@5: 0.1256131057484795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:07<00:00, 239.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@1: 0.03825779870512066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:07<00:00, 241.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5: 0.08228435972107628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:06<00:00, 277.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1: 0.03825779870512066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:06<00:00, 276.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5: 0.07119874435942712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' with social edges\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### METRIC EVAL #######\n",
    "\n",
    "def precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_labels = ground_truth[top_k_indices]\n",
    "    num_ones = np.sum(ground_truth == 1)\n",
    "    hit = np.sum(top_k_labels > 0)\n",
    "    return hit / min(num_ones, k) # k\n",
    "\n",
    "def average_hit_at_k(k, ground_truth, pred, user_ids, edge_index):\n",
    "    precisions = []\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        \n",
    "        precisions.append(\n",
    "            precision_at_k(user_id, sorted_indices, filtered_ground_truth, k)\n",
    "        )\n",
    "        \n",
    "    return np.mean(precisions)\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute DCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - DCG@k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k for a list of relevance scores\n",
    "    \n",
    "    Parameters:\n",
    "    - r: Relevance scores in rank order\n",
    "    - k: Rank\n",
    "    \n",
    "    Returns:\n",
    "    - NDCG@k\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "def calculate_ndcg_at_k(k, ground_truth, pred, edge_index):\n",
    "    \"\"\"\n",
    "    Calculate the average NDCG@k for all users\n",
    "    \n",
    "    Parameters:\n",
    "    - k: Rank\n",
    "    - ground_truth: True relevance scores\n",
    "    - pred: Predicted relevance scores\n",
    "    - edge_index: User-item interaction indices\n",
    "    \n",
    "    Returns:\n",
    "    - Average NDCG@k\n",
    "    \"\"\"\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "    ndcgs = []\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        \n",
    "        # Sort by predicted score\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        sorted_ground_truth = filtered_ground_truth[sorted_indices]\n",
    "        \n",
    "        ndcgs.append(ndcg_at_k(sorted_ground_truth, k))\n",
    "        \n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "def average_precision_at_k(user_id, sorted_indices, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single user.\n",
    "    \n",
    "    Args:\n",
    "    user_id: The user id.\n",
    "    sorted_indices: Indices that would sort the predicted ratings.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    k: The number of recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "    The average precision at k for the given user.\n",
    "    \"\"\"\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_labels = ground_truth[top_k_indices]\n",
    "    \n",
    "    relevant_indices = np.where(top_k_labels > 0)[0]\n",
    "    num_relevant = len(relevant_indices)\n",
    "    \n",
    "    if num_relevant == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 0.0\n",
    "    for i in relevant_indices:\n",
    "        prec_at_i = np.sum(top_k_labels[:i+1]) / (i + 1)\n",
    "        score += prec_at_i\n",
    "    \n",
    "    return score / min(num_relevant, k)\n",
    "\n",
    "def mean_ap_at_k(k, ground_truth, pred, user_ids, edge_index):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    Args:\n",
    "    k: The number of recommendations to consider.\n",
    "    ground_truth: Actual ratings (binary) indicating whether an item is relevant or not.\n",
    "    pred: Predicted ratings.\n",
    "    \n",
    "    Returns:\n",
    "    The mean average precision at k over all users.\n",
    "    \"\"\"\n",
    "    \n",
    "    average_precisions = []\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "        mask = edge_index[0] == user_id\n",
    "        filtered_pred = pred[mask]\n",
    "        filtered_ground_truth = ground_truth[mask]\n",
    "        sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "        \n",
    "        average_precisions.append(\n",
    "            average_precision_at_k(user_id, sorted_indices, filtered_ground_truth, k)\n",
    "        )\n",
    "        \n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "def evaluate(k_values, test_data_all2all, ground_truth, pred):\n",
    "    edge_index = test_data_all2all['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "\n",
    "    for k in k_values:\n",
    "        ### HIT@K ###\n",
    "        hit_at_k = average_hit_at_k(k, ground_truth, pred, user_ids, edge_index)\n",
    "        print(f\"HIT@{k}: {hit_at_k}\")\n",
    "    for k in k_values:\n",
    "        ### NDCG@K ###\n",
    "        ndcg_result = calculate_ndcg_at_k(k, ground_truth, pred, edge_index)\n",
    "        print(f\"NDCG@{k}: {ndcg_result}\")\n",
    "    for k in k_values:\n",
    "        map_at_k = mean_ap_at_k(k, ground_truth, pred, user_ids, edge_index)\n",
    "        print(f\"MAP@{k}: {map_at_k}\")\n",
    "\n",
    "\n",
    "eval_loader = {\n",
    "    'gnn': {\n",
    "        'ground_truth': ground_truth_gnn,\n",
    "        'pred': pred_gnn\n",
    "    },\n",
    "    # 'nmf': {\n",
    "    #     'ground_truth': ground_truth_nmf,\n",
    "    #     'pred': pred_nmf\n",
    "    # },\n",
    "    # 'cmf': {\n",
    "    #     'ground_truth': ground_truth_cmf,\n",
    "    #     'pred': pred_cmf\n",
    "    # },\n",
    "\n",
    "}\n",
    "k_values = [1, 5]\n",
    "evaluate(k_values, test_data_all2all, ground_truth=eval_loader[model_variant_eval]['ground_truth'], pred=eval_loader[model_variant_eval]['pred'])\n",
    "''' without social edges\n",
    "HIT@1: 0.03825779870512066\n",
    "HIT@5: 0.1256131057484795\n",
    "NDCG@1: 0.03825779870512066\n",
    "NDCG@5: 0.08228435972107628\n",
    "MAP@1: 0.03825779870512066\n",
    "MAP@5: 0.07119874435942712\n",
    "'''\n",
    "''' with social edges\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DIVERSITY EXPERIMENT ##############\n",
    "if experiment == 'diversity':\n",
    "    edge_index = test_data_all2all['user', 'rates', 'item'].edge_label_index\n",
    "    user_ids = np.unique(edge_index[0].numpy())\n",
    "    pred = eval_loader[model_variant_eval]['pred']\n",
    "    ground_truth = eval_loader[model_variant_eval]['ground_truth']\n",
    "\n",
    "    for k in k_values:\n",
    "        recs_list = set()\n",
    "        for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "            mask = edge_index[0] == user_id\n",
    "            filtered_pred = pred[mask]\n",
    "            filtered_items = edge_index[1][mask]\n",
    "            sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "            top_k_indices = sorted_indices[:k]\n",
    "            top_k_indices = top_k_indices.copy()\n",
    "            top_k_items = filtered_items[top_k_indices].numpy()\n",
    "            recs_list.update(top_k_items)\n",
    "\n",
    "        diversity_at_k = len(recs_list) / len(np.unique(edge_index[1].numpy()))\n",
    "        print(f'Item coverage diversity for {model_variant_eval} @{k}:', diversity_at_k)\n",
    "    \n",
    "    for k in k_values:\n",
    "        users_with_relevant_recs = set()\n",
    "        \n",
    "        for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "            mask = edge_index[0] == user_id\n",
    "            filtered_pred = pred[mask]\n",
    "            sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "            top_k_indices = sorted_indices[:k]\n",
    "            filtered_ground_truth = ground_truth[mask] \n",
    "            relevant_recs = filtered_ground_truth[top_k_indices] \n",
    "            \n",
    "            if np.sum(relevant_recs) > 0:  # At least one relevant recommendation\n",
    "                users_with_relevant_recs.add(user_id)\n",
    "        \n",
    "        user_coverage_at_k = len(users_with_relevant_recs) / len(user_ids)\n",
    "        print(f'User coverage for {model_variant_eval} @{k}:', user_coverage_at_k)\n",
    "\n",
    "    #######  Intra-List Diversity #######\n",
    "    # TODO: Based on item_feat define the compute_dissimilarity method\n",
    "    # for k in k_values:\n",
    "    #     avg_dissimilarity = []\n",
    "        \n",
    "    #     for user_id in tqdm(user_ids, total=len(user_ids)):\n",
    "    #         mask = edge_index[0] == user_id\n",
    "    #         filtered_pred = pred[mask]\n",
    "    #         filtered_items = edge_index[1][mask]\n",
    "    #         sorted_indices = np.argsort(filtered_pred)[::-1]\n",
    "    #         top_k_indices = sorted_indices[:k]\n",
    "    #         top_k_items = filtered_items[top_k_indices].numpy()\n",
    "            \n",
    "    #         dissimilarity_sum = 0\n",
    "    #         for i in range(len(top_k_items)):\n",
    "    #             for j in range(i+1, len(top_k_items)):\n",
    "    #                 dissimilarity_sum += compute_dissimilarity(top_k_items[i], top_k_items[j])\n",
    "            \n",
    "    #         if k > 1:\n",
    "    #             avg_pairwise_dissimilarity = 2 * dissimilarity_sum / (k * (k - 1))\n",
    "    #             avg_dissimilarity.append(avg_pairwise_dissimilarity)\n",
    "        \n",
    "    #     intra_list_diversity_at_k = np.mean(avg_dissimilarity)\n",
    "    #     print(f'Intra-list diversity for {model_mode_eval} @{k}:', intra_list_diversity_at_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HIT@K EVAL V1 ##########\n",
    "# in val_data len(edge_index) = 80670, but len(edge_label_index) = 30249, we selected edge_label_index since for train_loader used the same\n",
    "def precision_at_k(user_id, edge_index, ground_truth, pred, k):\n",
    "\n",
    "    mask = edge_index[0] == user_id\n",
    "    filtered_pred = pred[mask]\n",
    "    filtered_ground_truth = ground_truth[mask]\n",
    "    sorted_indices = filtered_pred.argsort()[:: -1]\n",
    "\n",
    "    top_k = [(filtered_ground_truth[i], filtered_pred[i]) for i in sorted_indices[:k]]\n",
    "    hit = 0\n",
    "    for i in range(len(top_k)):\n",
    "        ground_truth, pred = top_k[i]\n",
    "        if ground_truth > 0 and pred > 0: # I think we should remove this: and pred > 0:\n",
    "            hit += 1\n",
    "    precision = hit / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ap_at_k(k, precision_at_k, mode):\n",
    "    precisions = []\n",
    "    edge_index = val_loader.data['user', 'rates', 'item'].edge_label_index\n",
    "    for user_id in tqdm(edge_index[0], total=len(edge_index[0])):\n",
    "        if mode == 'nmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_nmf, k)) # ground_truth is the same for both GNN and mf\n",
    "        if mode == 'cmf':\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred_cmf, k))\n",
    "        else:\n",
    "            precisions.append(precision_at_k(user_id, edge_index, ground_truth, pred, k))\n",
    "            break\n",
    "\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "for k in k_values:\n",
    "    hit_at_k = ap_at_k(k, precision_at_k, mode='GNN')\n",
    "    print(f\"AP@{k}:\", hit_at_k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
