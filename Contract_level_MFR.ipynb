{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### CONTRACT ADDRESSES IN USER_TXS DATASET ######################\n",
    "\n",
    "# Load your dataframes\n",
    "user_transactions = pd.read_csv('dataset/user_transactions.csv')\n",
    "contract_addresses = pd.read_csv('dataset/contract_addresses.csv')\n",
    "\n",
    "# Extract unique addresses from 'from' and 'to' columns\n",
    "unique_addresses = pd.concat([user_transactions['from'], user_transactions['to']]).unique()\n",
    "\n",
    "# Check if these addresses exist in contract_addresses\n",
    "matches = contract_addresses[contract_addresses['address'].isin(unique_addresses)]\n",
    "\n",
    "# Write the matches to a CSV file\n",
    "matches.to_csv('contract_addresses_in_user_txs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### UNIQUE CONTRACTS IN CONTRACTS_CODES.JSON ##################\n",
    "json_file = 'contract_parser/verified-smart-contracts/data/combined.json'\n",
    "\n",
    "contracts = set()\n",
    "counter = 0\n",
    "with open(json_file, 'r') as f:\n",
    "    for line in f:\n",
    "        counter = counter + 1\n",
    "        data = json.loads(line)\n",
    "        contracts.add((data['source_code'], data['address']))\n",
    "\n",
    "\n",
    "unique_source_codes = set()\n",
    "\n",
    "for contract in tqdm(contracts, desc=\"Processing\"):\n",
    "    unique_source_codes.add(contract[0])\n",
    "\n",
    "print(f\"The number of unique source codes is {len(unique_source_codes)}\")\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### LOAD PARSED CONTRACT HUGE JSON ###################\n",
    "with open('contract_parser/verified-smart-contracts/data/combined.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_contracts = len(data)\n",
    "unique_contracts = len(set(contract['contract_name'] for contract in data))\n",
    "\n",
    "print(f\"Total number of contracts: {total_contracts}\")\n",
    "print(f\"Number of unique contract names: {unique_contracts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ CREATE DF OF PARSED CONTRACTS ##########################\n",
    "dir_path = 'contract_parser/verified-smart-contracts/data/parsed'\n",
    "files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.parquet')]\n",
    "\n",
    "dfs = [pd.read_parquet(f) for f in files]\n",
    "\n",
    "parsed_contracts = pd.concat(dfs, ignore_index=True)\n",
    "print('columns of parsed contracts df:', parsed_contracts.columns) # so it just save all the classes of same contract separatly, so the only unique identifire would be the contract_name I guess\n",
    "class_counts = parsed_contracts['class_name'].value_counts()\n",
    "common_libraries = class_counts[:10].index.tolist()\n",
    "print('10 most common class_names in parsed contracts df:', common_libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### TOPIC MODELING ON CONTRACTS COMMENTS ##################################\n",
    "\n",
    "'''\n",
    "concatenate all class_documentations associated to one contract, \n",
    "remove commun function comments, so each doc is one contract all comments, \n",
    "run LDA\n",
    "Common_libraries = ['ERC721A', 'SafeMath', 'IUniswapV2Pair', 'ERC20', 'IERC20', 'ERC721', 'Ownable']\n",
    "'''\n",
    "\n",
    "common_libraries = []\n",
    "\n",
    "def concat_non_common_docs(docs, classes):\n",
    "    # This will run for each group of class_documentation strings in the same contract_name.\n",
    "    non_common_docs = [doc for doc, class_name in zip(docs, classes) if class_name not in common_libraries and pd.notnull(doc)]\n",
    "    return ' '.join(non_common_docs)\n",
    "\n",
    "contract_docs = parsed_contracts.groupby('contract_name').apply(lambda x: concat_non_common_docs(x['class_documentation'], x['class_name']))\n",
    "\n",
    "contract_docs = contract_docs.reset_index(name='class_documentation')\n",
    "\n",
    "print(len(contract_docs))\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\n",
    "\n",
    "processed_contracts = [preprocess_string(doc, CUSTOM_FILTERS) for doc in contract_docs['class_documentation']]\n",
    "\n",
    "# Create the dictionary and corpus required for LDA\n",
    "dictionary = corpora.Dictionary(processed_contracts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.1)  # adjust these parameters\n",
    "\n",
    "corpus = [dictionary.doc2bow(contract) for contract in processed_contracts]\n",
    "\n",
    "num_topics = 15  # Choose an appropriate number\n",
    "lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print(f\"Topic #{i}: \", lda.print_topic(i))\n",
    "\n",
    "'''\n",
    "29488\n",
    "Topic #0:  0.067*\"assumes\" + 0.038*\"support\" + 0.038*\"burning\" + 0.038*\"uint128\" + 0.038*\"mints\" + 0.037*\"e\" + 0.036*\"g\" + 0.036*\"at\" + 0.036*\"gas\" + 0.036*\"minted\"\n",
    "Topic #1:  0.071*\"enumerable\" + 0.067*\"but\" + 0.060*\"erc721enumerable\" + 0.059*\"separately\" + 0.047*\"type\" + 0.043*\"collection\" + 0.041*\"related\" + 0.032*\"compliant\" + 0.031*\"optional\" + 0.025*\"ids\"\n",
    "Topic #2:  0.058*\"note\" + 0.054*\"over\" + 0.053*\"8\" + 0.048*\"compiler\" + 0.048*\"now\" + 0.048*\"needed\" + 0.046*\"checking\" + 0.043*\"overflow\" + 0.043*\"wrappers\" + 0.043*\"arithmetic\"\n",
    "Topic #3:  0.031*\"notice\" + 0.028*\"author\" + 0.010*\"mint\" + 0.009*\"7\" + 0.009*\"minting\" + 0.009*\"nft\" + 0.008*\"time\" + 0.008*\"we\" + 0.007*\"at\" + 0.007*\"users\"\n",
    "Topic #4:  0.098*\"erc721a\" + 0.069*\"order\" + 0.065*\"optimized\" + 0.062*\"ids\" + 0.062*\"sequential\" + 0.061*\"assumptions\" + 0.026*\"hashing\" + 0.023*\"merkle\" + 0.021*\"proofs\" + 0.019*\"test\"\n",
    "Topic #5:  0.027*\"events\" + 0.026*\"t\" + 0.026*\"added\" + 0.026*\"applications\" + 0.026*\"how\" + 0.026*\"mechanisms\" + 0.025*\"ierc20\" + 0.014*\"openzeppelin\" + 0.014*\"allowance\" + 0.014*\"instead\"\n",
    "Topic #6:  0.040*\"overflow\" + 0.029*\"bugs\" + 0.028*\"o\" + 0.025*\"arithmetic\" + 0.021*\"added\" + 0.020*\"type\" + 0.015*\"reverting\" + 0.015*\"result\" + 0.015*\"overflows\" + 0.015*\"recommended\"\n",
    "Topic #7:  0.122*\"pragma\" + 0.079*\"experimental\" + 0.078*\"abiencoderv2\" + 0.077*\"sol\" + 0.070*\"import\" + 0.068*\"openzeppelin\" + 0.047*\"utils\" + 0.046*\"math\" + 0.041*\"lib\" + 0.030*\"8\"\n",
    "Topic #8:  0.064*\"role\" + 0.045*\"admin\" + 0.028*\"roles\" + 0.026*\"function\" + 0.023*\"proxy\" + 0.018*\"call\" + 0.014*\"accounts\" + 0.013*\"public\" + 0.013*\"my\" + 0.010*\"or\"\n",
    "Topic #9:  0.074*\"assumes\" + 0.074*\"max\" + 0.073*\"cannot\" + 0.072*\"value\" + 0.038*\"than\" + 0.035*\"maximum\" + 0.034*\"id\" + 0.034*\"exceed\" + 0.034*\"64\" + 0.034*\"uint64\"\n",
    "Topic #10:  0.114*\"███\" + 0.080*\"pool\" + 0.068*\"░\" + 0.045*\"██\" + 0.041*\"eth\" + 0.039*\"staking\" + 0.027*\"reward\" + 0.027*\"rewards\" + 0.023*\"▓▓\" + 0.022*\"▒\"\n",
    "Topic #11:  0.055*\"you\" + 0.023*\"safeerc20\" + 0.023*\"throw\" + 0.020*\"call\" + 0.020*\"failure\" + 0.019*\"returns\" + 0.018*\"or\" + 0.018*\"name\" + 0.018*\"value\" + 0.018*\"return\"\n",
    "Topic #12:  0.031*\"asset\" + 0.023*\"each\" + 0.022*\"proxy\" + 0.021*\"ether\" + 0.020*\"caller\" + 0.016*\"t\" + 0.016*\"swap\" + 0.013*\"then\" + 0.013*\"split\" + 0.013*\"made\"\n",
    "Topic #13:  0.061*\"or\" + 0.045*\"license\" + 0.039*\"io\" + 0.028*\"software\" + 0.024*\"synthetix\" + 0.024*\"source\" + 0.022*\"interfaces\" + 0.021*\"any\" + 0.019*\"c\" + 0.017*\"copyright\"\n",
    "Topic #14:  0.071*\"com\" + 0.061*\"github\" + 0.048*\"author\" + 0.031*\"blob\" + 0.028*\"sol\" + 0.027*\"master\" + 0.024*\"20\" + 0.019*\"code\" + 0.019*\"smart\" + 0.016*\"erc\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of contracts classes:',len(parsed_contracts))\n",
    "unique_contract_names = parsed_contracts['contract_name'].unique().tolist()\n",
    "print('number of unique contracts by their names:',len(unique_contract_names))\n",
    "print('some sample of contract_names:',unique_contract_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## TAG CONTRACTS BY TRAINED LDA #######################\n",
    "#TODO: Fix ths bug \n",
    " \n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda, corpus=corpus, texts=processed_contracts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Contract_Name', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Merge with the original data\n",
    "contract_name_to_topic_df = pd.concat([contract_docs['contract_name'], df_dominant_topic], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "contract_name_to_topic_df.to_csv('contract_name_to_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CREATE new adj_matrix with topic modeling tagging, re-run CF recommender and compare result with Kmean tagging model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
