{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymongo web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm pandas pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "# from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "# Download the contracts collection directly from MongoDB\n",
    "client = MongoClient('mongodb://seshatadmin:uWBOzDTQLXJLiFFF@lg-research-1.uwaterloo.ca:8094/')\n",
    "db = client['test']\n",
    "collection = db['contracts']\n",
    "\n",
    "# Retrieve all documents in the collection\n",
    "results = collection.find()\n",
    "\n",
    "# Convert the cursor to a list of dictionaries\n",
    "documents = list(results)\n",
    "\n",
    "# Save the documents as JSON in a file\n",
    "with open('dataset/contracts.json', 'w') as file:\n",
    "    json.dump(documents, file)\n",
    "\n",
    "print(\"Data saved as contracts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Adjacency Matrix: 100%|██████████| 200001/200001 [05:00<00:00, 665.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "# Create an empty adjacency matrix\n",
    "adj_matrix = np.zeros((len(eoa_accounts), len(contract_accounts)))\n",
    "\n",
    "# Iterate over each row in the DataFrame and update adjacency matrix\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating Adjacency Matrix\"):\n",
    "    from_address = row['from']\n",
    "    to_address = row['to']\n",
    "    \n",
    "    if from_address in eoa_accounts and to_address in contract_accounts:\n",
    "        from_index = eoa_accounts.index(from_address)\n",
    "        to_index = contract_accounts.index(to_address)\n",
    "        adj_matrix[from_index, to_index] = 1\n",
    "\n",
    "# Create DataFrames from the adjacency matrix and the categorized addresses\n",
    "eoa_contract_adj_df = pd.DataFrame(adj_matrix, index=eoa_accounts, columns=contract_accounts)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "eoa_contract_adj_df.to_csv('dataset/adjacency_matrix.csv', index=True)\n",
    "\n",
    "print(\"number of unique contracts within from and to of transactions\", eoa_contract_adj_df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data into a pandas DataFrame\n",
    "with open('dataset/transactions.json') as f:\n",
    "    # data = [json.loads(line) for line in f]\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "column_names = df.columns.to_list()\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete = [col for col in df.columns if col.startswith('func_args')]\n",
    "df = df.drop(columns=columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['from', 'to']\n",
    "df = df.drop(columns=df.columns.difference(columns_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['from'] = df['from'].astype(str)\n",
    "df['to'] = df['to'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get unique Ethereum public keys from 'from' and 'to' columns\n",
    "unique_addresses = np.unique(np.concatenate([df['from'].unique(), df['to'].unique()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92045\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorizing Addresses: 100%|██████████| 92045/92045 [00:00<00:00, 1047824.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the contracts.json file\n",
    "with open('dataset/contracts.json') as file:\n",
    "    contracts_data = json.load(file)\n",
    "\n",
    "# Create a set of contract addresses for faster lookup\n",
    "contract_addresses = set(contract['contractAddress'] for contract in contracts_data)\n",
    "\n",
    "# Create empty arrays for EOA accounts (rows) and contract accounts (columns)\n",
    "eoa_accounts = []\n",
    "contract_accounts = []\n",
    "\n",
    "# Iterate over unique addresses and categorize them\n",
    "for address in tqdm(unique_addresses, desc=\"Categorizing Addresses\"):\n",
    "    if address in contract_addresses:\n",
    "        contract_accounts.append(address)\n",
    "    else:\n",
    "        eoa_accounts.append(address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique addresses saved as unique_addresses.csv\n"
     ]
    }
   ],
   "source": [
    "address_df = pd.DataFrame(unique_addresses, columns=['Address'])\n",
    "\n",
    "# Save as a CSV file\n",
    "address_df.to_csv('unique_addresses.csv', index=False)\n",
    "\n",
    "print(\"Unique addresses saved as unique_addresses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Contract Addresses: 100%|█████████▉| 7214791/7214792 [00:10<00:00, 686531.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOA addresses saved to eoa_addresses.csv.\n"
     ]
    }
   ],
   "source": [
    "# The script to go over unique addresses, lookup within contract_addresses, \n",
    "# if not exist, append to the EoA_addresses and save the csv\n",
    "\n",
    "# Read unique addresses from unique_addresses.csv\n",
    "unique_addresses = set()\n",
    "with open('dataset/unique_addresses.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        address = row['Address']\n",
    "        unique_addresses.add(address)\n",
    "\n",
    "# Read contract addresses from contract_addresses.csv\n",
    "contract_addresses = set()\n",
    "with open('dataset/contract_addresses.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    total_lines = sum(1 for _ in file)  # Count total lines in the file\n",
    "    file.seek(0)  # Reset file position\n",
    "    progress_bar = tqdm(reader, total=total_lines, desc=\"Processing Contract Addresses\")\n",
    "    for row in progress_bar:\n",
    "        address = row['address']\n",
    "        contract_addresses.add(address)\n",
    "\n",
    "# Find EOA addresses\n",
    "eoa_addresses = unique_addresses - contract_addresses\n",
    "\n",
    "# Append EOA addresses to eoa_addresses.csv\n",
    "fieldnames = ['address']\n",
    "with open('dataset/eoa_addresses.csv', 'a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    if file.tell() == 0:\n",
    "        writer.writeheader()\n",
    "    for address in eoa_addresses:\n",
    "        writer.writerow({'address': address})\n",
    "\n",
    "print(\"EOA addresses saved to eoa_addresses.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84234\n"
     ]
    }
   ],
   "source": [
    "print(len(eoa_addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "#Clean the data from fetch_tx.py (prev version) to add top level address key to each normal txs array\n",
    "import json\n",
    "import os\n",
    "\n",
    "# specify the directory you want to parse json files from\n",
    "directory = 'temp_data'\n",
    "\n",
    "# iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Open each json file\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            data_list = json.load(f)\n",
    "\n",
    "        updated_data_list = []\n",
    "        for data in data_list:\n",
    "            # Check if 'normal' key exists in json\n",
    "            if 'normal' in data:\n",
    "                normal_data = data['normal']\n",
    "                if len(normal_data) > 0 and 'from' in normal_data[0]:\n",
    "                    # Fetch the 'from' address of the first entry\n",
    "                    address_from = normal_data[0]['from']\n",
    "                    address_to = normal_data[0]['to']\n",
    "\n",
    "                    # Verify if the same address is present in 'to' or 'from' of all other entries\n",
    "                    if all(entry.get('from', '') == address_from or entry.get('to', '') == address_from for entry in normal_data):\n",
    "                        # If true, then insert this address as a key in the json object\n",
    "                        data = {address_from: data}\n",
    "                    else:\n",
    "                        data = {address_to: data}\n",
    "            updated_data_list.append(data)\n",
    "\n",
    "        # Write the updated json to a new file\n",
    "        with open(os.path.join(directory, filename.split('.')[0] + '_updated.json'), 'w') as f:\n",
    "            json.dump(updated_data_list, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 files.\n"
     ]
    }
   ],
   "source": [
    "# When stop tx_fetcher, re-run these four steps before running the script again\n",
    "# Step0: merging user-tx dataset json files\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory = \"temp_tx\"\n",
    "\n",
    "# Initialize an empty dictionary to store the merged data\n",
    "merged_data = {}\n",
    "\n",
    "# Initialize a counter for skipped files\n",
    "skipped = 0\n",
    "\n",
    "# Initialize a list to store the paths of successfully processed files\n",
    "processed_files = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            try:\n",
    "                data_list = json.load(file)\n",
    "                \n",
    "                # Iterate over each dictionary in the list\n",
    "                for data in data_list:\n",
    "                    # Merge the data into the merged_data dictionary\n",
    "                    merged_data.update(data)\n",
    "                \n",
    "                # Add the path of the successfully processed file to the list\n",
    "                processed_files.append(file_path)\n",
    "            except ValueError as e:\n",
    "                # If an error occurs, increment the skipped counter and continue\n",
    "                skipped += 1\n",
    "                print(f\"Skipping file {filename} due to error: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "print(f\"Skipped {skipped} files.\")\n",
    "\n",
    "# Write the merged data to a new file\n",
    "output_file = \"dataset/merged_user_transactions.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(merged_data, file, indent=4)\n",
    "\n",
    "# Delete the successfully processed files\n",
    "for file_path in processed_files:\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step1\n",
    "#To extract all the top level keys (addresses) from new fetched txs\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# specify the directory you want to parse json files from\n",
    "directory = 'dataset'\n",
    "filename = 'merged_user_transactions.json'\n",
    "\n",
    "\n",
    "with open(os.path.join(directory, filename), 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "addresses = data_list.keys()\n",
    "\n",
    "# If you want to convert it to a list\n",
    "addresses_list = list(addresses)\n",
    "\n",
    "# Write the list of addresses to a csv file\n",
    "with open('dataset/processed_eoa_addresses.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['address'])\n",
    "    for address in addresses_list:\n",
    "        writer.writerow([address])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2, TODO: just delete the latest one and rename the new_... to latest_...\n",
    "#To create the local_remaining_addresses, just finding all addresses that don't exist in processed.csv and exists in remaining.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from csv files\n",
    "remaining_addresses_df = pd.read_csv('dataset/latest_remaining_eoa_addresses.csv')\n",
    "processed_addresses_df = pd.read_csv('dataset/processed_eoa_addresses.csv')\n",
    "\n",
    "# Find addresses that are in addresses.csv but not in remaining_addresses.csv\n",
    "difference_df = remaining_addresses_df.loc[~remaining_addresses_df['address'].isin(processed_addresses_df['address'])]\n",
    "\n",
    "# Save these addresses to a new csv file\n",
    "difference_df.to_csv('dataset/new_remaining_eoa_addresses.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "# update user-tx csv dataset\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load the merged data\n",
    "with open(\"dataset/merged_user_transactions.json\", \"r\") as file:\n",
    "    merged_data = json.load(file)\n",
    "\n",
    "# Open a new CSV file for appending\n",
    "with open(\"dataset/user_transactions.csv\", \"a\", newline='') as file:\n",
    "    # Define the fieldnames for the CSV\n",
    "    fieldnames = [\"address\", \"timeStamp\", \"from\", \"to\", \"value\", \"gas\", \"gasPrice\", \"input\", \"contractAddress\", \"methodId\", \"functionName\"]\n",
    "    \n",
    "    # Create a CSV writer\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # If file doesn't exist, write the header row\n",
    "    if os.stat(\"dataset/user_transactions.csv\").st_size == 0:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Initialize a counter for skipped addresses\n",
    "    skipped_addresses = 0\n",
    "    \n",
    "    # Iterate over the merged data\n",
    "    for address, data in merged_data.items():\n",
    "        # Check if data[\"normal\"] is a list\n",
    "        if isinstance(data, dict) and \"normal\" in data and isinstance(data[\"normal\"], list):\n",
    "            # For each address, iterate over the transactions\n",
    "            for tx in data[\"normal\"]:\n",
    "                # Create a row for each transaction\n",
    "                row = {\n",
    "                    \"address\": address,\n",
    "                    \"timeStamp\": tx.get(\"timeStamp\", \"\"),\n",
    "                    \"from\": tx.get(\"from\", \"\"),\n",
    "                    \"to\": tx.get(\"to\", \"\"),\n",
    "                    \"value\": tx.get(\"value\", \"\"),\n",
    "                    \"gas\": tx.get(\"gas\", \"\"),\n",
    "                    \"gasPrice\": tx.get(\"gasPrice\", \"\"),\n",
    "                    \"input\": tx.get(\"input\", \"\"),\n",
    "                    \"contractAddress\": tx.get(\"contractAddress\", \"\"),\n",
    "                    \"methodId\": tx.get(\"methodId\", \"\"),\n",
    "                    \"functionName\": tx.get(\"functionName\", \"\")\n",
    "                }\n",
    "                \n",
    "                # Write the row to the CSV\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            # Increment the counter of skipped addresses\n",
    "            skipped_addresses += 1\n",
    "\n",
    "# Print the number of skipped addresses\n",
    "print(f\"Skipped addresses: {skipped_addresses}\")\n",
    "\n",
    "# Remove the temp merged_tx dataset \n",
    "os.remove(\"dataset/merged_user_transactions.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator between eoa and contract addresses\n",
    "# We checked for some of eoa_addresses and found all of them was eoa addresses (but we can run for all remaining ones)\n",
    "import requests \n",
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_if_contract(address):\n",
    "    url = \"https://mainnet.infura.io/v3/8f890b3a78e740f2bd98be613da634f1\"  # URL of your Ethereum node\n",
    "\n",
    "    payload = {\n",
    "        \"method\": \"eth_getCode\",\n",
    "        \"params\": [address, \"latest\"],\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\"\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    result = response.json()['result']\n",
    "\n",
    "    return result != '0x'\n",
    "\n",
    "# Open the CSV file and read it into memory\n",
    "with open('dataset/eoa_addresses.csv', 'r') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    headers = next(reader)  # Extract header row\n",
    "    data = list(reader)\n",
    "\n",
    "# Add the 'is_eoa' column if it doesn't exist\n",
    "if 'is_eoa' not in headers:\n",
    "    headers.append('is_eoa')\n",
    "    for row in data:\n",
    "        row.append('')  # Initialize with an empty string or any other default value\n",
    "\n",
    "# Find the index of the is_eoa column\n",
    "is_eoa_index = headers.index('is_eoa')\n",
    "\n",
    "# Open the output file\n",
    "with open('dataset/checked_eoa_addresses.csv', 'w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(headers)  # Write the header row\n",
    "\n",
    "    # Iterate over the data and check the is_eoa value for each address\n",
    "    for row in tqdm(data, desc=\"Processing addresses\"):\n",
    "        if row[is_eoa_index]:  # Skip if is_eoa is already populated\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        # Check if address is a contract\n",
    "        is_contract = check_if_contract(row[0])\n",
    "        # Update is_eoa field in the row\n",
    "        row[is_eoa_index] = 0 if is_contract else 1\n",
    "        # Write the updated row to the output file\n",
    "        writer.writerow(row)\n",
    "\n",
    "# prev progress bar: Processing addresses:   0%|          | 56/84234 [01:17<32:22:51,  1.38s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions: 100%|██████████| 20000/20000 [00:02<00:00, 8184.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Shape: (20000, 2777)\n"
     ]
    }
   ],
   "source": [
    "#Create an adjacency matrix \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the data from CSV files\n",
    "user_tx = pd.read_csv('dataset/user_transactions.csv')\n",
    "# Get the first 20,000 rows\n",
    "user_tx = user_tx.head(20000)\n",
    "all_contract_addresses = pd.read_csv('dataset/contract_addresses.csv')\n",
    "\n",
    "# Find unique addresses\n",
    "unique_addresses = pd.concat([user_tx['address'], user_tx['from'], user_tx['to']]).unique()\n",
    "\n",
    "\n",
    "# Convert all_contract_addresses['address'] to set for faster lookup\n",
    "all_contract_addresses_set = set(all_contract_addresses['address'])\n",
    "\n",
    "# Filter unique_addresses\n",
    "contract_addresses = [address for address in unique_addresses if address in all_contract_addresses_set]\n",
    "\n",
    "# Convert contract_addresses to pandas series and reset its index\n",
    "contract_addresses = pd.Series(contract_addresses).reset_index(drop=True)\n",
    "\n",
    "#Create an empty adjacency matrix\n",
    "adj_matrix = np.zeros((len(user_tx), len(contract_addresses)))\n",
    "\n",
    "# Convert contract_addresses to a set for faster lookup\n",
    "contract_addresses_set = set(contract_addresses)\n",
    "\n",
    "# Loop through transactions and increment matrix cell for each transaction\n",
    "for i in tqdm(range(len(user_tx)), desc=\"Processing transactions\"):\n",
    "    from_address = user_tx['from'][i]\n",
    "    to_address = user_tx['to'][i]\n",
    "\n",
    "    if from_address in contract_addresses_set:\n",
    "        j = contract_addresses[contract_addresses == from_address].index[0]\n",
    "        adj_matrix[i][j] = 1\n",
    "    elif to_address in contract_addresses_set:\n",
    "        j = contract_addresses[contract_addresses == to_address].index[0]\n",
    "        adj_matrix[i][j] = 1\n",
    "\n",
    "# Print the shape of the `adj_matrix`\n",
    "print(\"Matrix Shape:\", adj_matrix.shape)\n",
    "\n",
    "# Save the adjacency matrix as a CSV file\n",
    "np.savetxt(\"adj_matrix.csv\", adj_matrix, delimiter=\",\")\n",
    "\n",
    "# Prev adj-matrix shapes -> 1.2M,50K (1.2M txs, and 50K contracts) \n",
    "# Run the state of the art CF (matrix factorization), \n",
    "# For thesis you should say how the environment is different (it's programetical environment) to make it obvious why it's not another recommendations problem,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix Factorization implemmentation of collaborative filtering\n",
    "import numpy as np\n",
    "\n",
    "def matrix_factorization(R, K, steps, alpha, beta):\n",
    "    # R: User-Item interaction matrix\n",
    "    # K: Number of latent factors\n",
    "    # steps: Number of iterations for optimization\n",
    "    # alpha: Learning rate\n",
    "    # beta: Regularization parameter\n",
    "\n",
    "    N, M = R.shape\n",
    "    P = np.random.rand(N, K)  # User matrix initialization\n",
    "    Q = np.random.rand(M, K)  # Item matrix initialization\n",
    "\n",
    "    for step in range(steps):\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                if R[i, j] > 0:\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :])\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - beta * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - beta * Q[j, k])\n",
    "\n",
    "        error = 0\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                if R[i, j] > 0:\n",
    "                    error += pow(R[i, j] - np.dot(P[i, :], Q[j, :]), 2)\n",
    "                    for k in range(K):\n",
    "                        error += (beta / 2) * (pow(P[i, k], 2) + pow(Q[j, k], 2))\n",
    "\n",
    "        if error < 0.001:\n",
    "            break\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "# Example usage\n",
    "# Assume R is the user-item interaction matrix\n",
    "\n",
    "N = 100  # Number of users\n",
    "M = 50   # Number of items\n",
    "K = 10   # Number of latent factors\n",
    "\n",
    "R = np.random.randint(6, size=(N, M))  # Example user-item interaction matrix\n",
    "P, Q = matrix_factorization(R, K, steps=100, alpha=0.01, beta=0.02)\n",
    "\n",
    "# Make predictions for user-item pairs\n",
    "user_id = 0\n",
    "item_id = 0\n",
    "prediction = np.dot(P[user_id, :], Q[item_id, :])\n",
    "\n",
    "print(f\"Prediction for user {user_id} and item {item_id}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load adjacency matrix\n",
    "data_df = pd.read_csv('adj_matrix.csv', header=None)\n",
    "n_users = data_df.shape[0]\n",
    "n_items = data_df.shape[1]\n",
    "\n",
    "# Convert the DataFrame to the format that the surprise package can read\n",
    "df = pd.DataFrame(np.column_stack((np.repeat(np.arange(n_users), n_items),\n",
    "                                   np.tile(np.arange(n_items), n_users),\n",
    "                                   data_df.values.flatten())),\n",
    "                  columns=['userID', 'itemID', 'rating'])\n",
    "\n",
    "reader = Reader(rating_scale=(0, 1))  # rating scale is from 0 to 1 (non-interactive to interactive)\n",
    "\n",
    "# Load the data from the file using the surprise package\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split into train and test set, here we use 80/20 split\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Use the SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict for the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Compute and print Root Mean Squared Error and Mean Absolute Error\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Predict for a specific user (user with userID 10 for example)\n",
    "user_id = 10\n",
    "user_predictions = []\n",
    "for item_id in range(n_items):\n",
    "    pred = algo.predict(user_id, item_id)\n",
    "    user_predictions.append(pred.est)\n",
    "\n",
    "print(\"Predicted ratings:\", user_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic modeling on contract comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
