{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankhatiri/sean2/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, stem_text\n",
    "from gensim.parsing.preprocessing import split_alphanum, remove_stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from surprise import KNNBasic # Not working on Apple silicon chip\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import cross_validate, KFold, train_test_split\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from neo4j import GraphDatabase\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40501\n",
      "model fit Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 56/5615 [11:37<2:52:16,  1.86s/it]  "
     ]
    }
   ],
   "source": [
    "############# RUNNING BASELINE RECOMMENDERS ###############\n",
    "\n",
    "data = pd.read_parquet('dataset/user_contract_rating.parquet')\n",
    "###### DATA PREPROCESSING #######\n",
    "def apply_rating_scale(rating):\n",
    "    if rating == 1:\n",
    "        return 1\n",
    "    elif rating <= 5:\n",
    "        return 2\n",
    "    elif rating <= 15:\n",
    "        return 3\n",
    "    elif rating <= 30:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "data['rating'] = data['rating'].apply(apply_rating_scale)\n",
    "\n",
    "\n",
    "################# AUTO-EVAL SVD CF #####################\n",
    "# algo = SVD()\n",
    "# print('cf started')\n",
    "# cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "################# AUTO-EVAL KNN CF #####################\n",
    "# sim_options = {\n",
    "#     'name': 'cosine',\n",
    "#     'user_based': True\n",
    "# }\n",
    "# algo = KNNBasic(sim_options=sim_options)\n",
    "# cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "#################### CF RECOMMENDER #######################\n",
    "'''The average percision just tell us how relevant our recommendations are, \n",
    "    but dosn't tell us how good our ranking is\n",
    "'''\n",
    "def MAP_at_K_MF_batch(model=None, data=None, K=None, batch_size=100):\n",
    "    total_map = 0.0\n",
    "    count_users = 0\n",
    "    \n",
    "    user_ids = [uid for uid, _, _ in data]\n",
    "    unique_user_ids = list(set(user_ids))\n",
    "    \n",
    "    # Divide unique_user_ids into batches\n",
    "    batches = [unique_user_ids[i:i + batch_size] for i in range(0, len(unique_user_ids), batch_size)]\n",
    "    \n",
    "    def process_batch(batch):\n",
    "        # print('batch processing started')\n",
    "        nonlocal total_map, count_users\n",
    "        item_ids = [iid for _, iid, _ in data]\n",
    "        \n",
    "        batch_map = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for uid in batch:\n",
    "            # print(f'processing user {uid} started')\n",
    "            true_relevant = [iid for (u, iid, r) in data if u == uid]\n",
    "            if len(true_relevant) == 0:\n",
    "                continue\n",
    "            \n",
    "            predicted_items = get_prediction(K, uid, item_ids, model)\n",
    "            hits = 0\n",
    "            sum_precisions = 0.0\n",
    "            \n",
    "            for k in range(1, K + 1):\n",
    "                item_at_k = predicted_items[k - 1]\n",
    "                if item_at_k in true_relevant:\n",
    "                    hits += 1\n",
    "                    precision_at_k = hits / k\n",
    "                    sum_precisions += precision_at_k\n",
    "                    \n",
    "            average_precision = sum_precisions / min(len(true_relevant), K)\n",
    "            batch_map += average_precision\n",
    "            batch_count += 1\n",
    "            # print(f'processing user {uid} finished')\n",
    "        \n",
    "        # print('batch processing finished')\n",
    "        return batch_map, batch_count\n",
    "    \n",
    "    def get_prediction(K, uid, item_ids, model):\n",
    "        predicted_scores = [model.predict(uid, iid).est for iid in item_ids]\n",
    "        top_k_indices = np.argsort(predicted_scores)[::-1][:K]\n",
    "        results = [item_ids[i] for i in top_k_indices]\n",
    "        return results\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(process_batch, batches), total=len(batches)))\n",
    "    \n",
    "    for batch_map, batch_count in results:\n",
    "        total_map += batch_map\n",
    "        count_users += batch_count\n",
    "    \n",
    "    MAP_at_K = total_map / count_users if count_users > 0 else 0\n",
    "    return MAP_at_K\n",
    "\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "data = Dataset.load_from_df(data[['user', 'item', 'rating']], reader)\n",
    "# data = Dataset.load_builtin('ml-100k')\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "print(trainset.n_users)\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "print('model fit Finished')\n",
    "K_values = [1]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(MAP_at_K_MF_batch, model=model, data=testset, K=k, batch_size=5): k for k in K_values}\n",
    "    for future in as_completed(futures):\n",
    "        k = futures[future]\n",
    "        try:\n",
    "            map_result = future.result()\n",
    "            print(f'MAP @ {k}: {map_result}')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to compute MAP @ {k} due to {e}')\n",
    "\n",
    "############# POPULAR CONTRACT RECOMMENDER ###############\n",
    "# def MAP_at_K_PCR(data=None, K=None):\n",
    "#     total_map = 0.0\n",
    "#     count_users = 0\n",
    "    \n",
    "#     for user in data['user'].unique():\n",
    "#         true_items = data[data['user'] == user]['item'].tolist()\n",
    "#         if len(true_items) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         predicted_items = data.groupby('item')['rating'].sum().sort_values(ascending=False).head(K).index.tolist()\n",
    "#         hits = 0\n",
    "#         sum_precisions = 0.0\n",
    "#         for k in range(1, K+1):\n",
    "#             item_at_k = predicted_items[k - 1]\n",
    "#             if item_at_k in true_items:\n",
    "#                 hits += 1\n",
    "#                 precision_at_k = hits / k\n",
    "#                 sum_precisions += precision_at_k\n",
    "                \n",
    "#         average_precision = sum_precisions / min(len(true_items), K)\n",
    "#         total_map += average_precision\n",
    "#         count_users += 1\n",
    "    \n",
    "#     MAP_at_K = total_map / count_users if count_users > 0 else 0\n",
    "#     return MAP_at_K\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = {executor.submit(MAP_at_K_PCR, data=data, K=k): k for k in K_values}\n",
    "\n",
    "#     for future in as_completed(futures):\n",
    "#         k = futures[future]\n",
    "#         try:\n",
    "#             map_result = future.result()\n",
    "#             print(f'MAP @ {k}: {map_result}')\n",
    "#         except Exception as e:\n",
    "#             print(f'Failed to compute MAP @ {k} due to {e}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sean2",
   "language": "python",
   "name": "sean2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
