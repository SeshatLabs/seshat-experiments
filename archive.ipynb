{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# MF CF IMPLEMMENTATION FROM SCRATCH ##############\n",
    "\n",
    "# Simulated user-item rating matrix\n",
    "# Rows: users\n",
    "# Columns: items\n",
    "# 0 indicates missing values\n",
    "R_df = pd.read_parquet(\"dataset/adj_matrix_contract_names.parquet\")\n",
    "R = R_df.values\n",
    "\n",
    "# Initialize parameters\n",
    "num_users, num_items = R.shape\n",
    "num_features = 3  # Number of latent features\n",
    "learning_rate = 0.01  # Learning rate\n",
    "regularization = 0.01  # Regularization parameter\n",
    "num_epochs = 100  # Number of epochs\n",
    "\n",
    "# Initialize user and item matrices with random values\n",
    "U = np.random.randn(num_users, num_features)\n",
    "I = np.random.randn(num_items, num_features)\n",
    "\n",
    "print('1')\n",
    "# Training\n",
    "for epoch in tqdm(range(num_epochs), total = num_epochs):\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if R[i][j] > 0:\n",
    "                # Compute error\n",
    "                eij = R[i][j] - np.dot(U[i, :], I[j, :].T)\n",
    "                \n",
    "                # Update matrices\n",
    "                for k in range(num_features):\n",
    "                    U[i][k] += learning_rate * (2 * eij * I[j][k] - regularization * U[i][k])\n",
    "                    I[j][k] += learning_rate * (2 * eij * U[i][k] - regularization * I[j][k])\n",
    "    \n",
    "    print('2')\n",
    "    # Compute total error\n",
    "    error = 0\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if R[i][j] > 0:\n",
    "                error += (R[i][j] - np.dot(U[i, :], I[j, :].T)) ** 2\n",
    "                for k in range(num_features):\n",
    "                    error += (regularization / 2) * (U[i][k] ** 2 + I[j][k] ** 2)\n",
    "    print('3')\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs} - Error: {error}\")\n",
    "\n",
    "# Create the prediction matrix\n",
    "prediction = np.dot(U, I.T)\n",
    "\n",
    "# print(\"Prediction matrix:\")\n",
    "# print(prediction)\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r, k):\n",
    "    r = np.asarray(r)[:k]\n",
    "    out = [precision_at_k(r, j + 1) for j, rel in enumerate(r) if rel]\n",
    "    if not out:\n",
    "        return 0\n",
    "    return np.mean(out)\n",
    "\n",
    "def mean_average_precision(rs, k):\n",
    "    return np.mean([average_precision(r, k) for r in rs])\n",
    "\n",
    "# Assume these are your predicted ratings\n",
    "predicted_ratings = prediction\n",
    "\n",
    "# Assume these are your true binary relevance scores (e.g., 1 if relevant, 0 otherwise)\n",
    "true_relevance = pd.read_parquet(\"dataset/adj_matrix_contract_names.parquet\")\n",
    "\n",
    "# Convert predicted ratings to binary relevance scores based on a threshold (optional)\n",
    "predicted_relevance = (predicted_ratings > 0.5).astype(int)\n",
    "\n",
    "# Sort the true relevance labels based on predicted ratings (higher ratings are assumed to be more relevant)\n",
    "sorted_relevance = []\n",
    "for i in range(predicted_ratings.shape[0]):\n",
    "    sorted_relevance.append(true_relevance[i, np.argsort(predicted_ratings[i])[::-1]])\n",
    "\n",
    "# Calculate MAP@K\n",
    "K = 5\n",
    "result = mean_average_precision(sorted_relevance, K)\n",
    "print(f\"MAP@{K}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PRECISION & RECALL EVAL ######################\n",
    "def precision_recall_at_k(predictions, k, threshold):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    user_est_true = defaultdict(list)\n",
    "\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings) \n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k]) \n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0 \n",
    "\n",
    "    return precisions, recalls, k"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
